import asyncio
import json
import re
import math  
import time
from datetime import datetime, timezone, timedelta
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

from dataclasses import dataclass
from typing import Optional, Dict, List
from telethon import TelegramClient, events
import logging
logger = logging.getLogger(__name__)

import aiosqlite
import sqlite3


# Nustatome logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)



class AsyncDatabase:
    def __init__(self):
        self.db_path = Config.DB_PATH
        self.conn = None
        self.setup_done = False
                
    async def connect(self):
        if not self.conn:
            try:
                self.conn = await aiosqlite.connect(self.db_path)
                self.conn.row_factory = aiosqlite.Row
                if not self.setup_done:
                    await self._setup_database()
                    self.setup_done = True
                logger.info(f"[{datetime.now(timezone.utc)}] Connected to database: {self.db_path}")
            except Exception as e:
                logger.error(f"[{datetime.now(timezone.utc)}] Database connection error: {e}")
                raise
                
    async def _setup_database(self):
        """Sukuria reikalingas lenteles jei j≈≥ nƒóra"""
        if self.setup_done:
            return
            
        try:
            # Token pradinƒó b≈´sena
            await self.execute("""
                CREATE TABLE IF NOT EXISTS token_initial_states (
                    address TEXT PRIMARY KEY,
                    name TEXT,
                    symbol TEXT,
                    age TEXT,
                    market_cap REAL,
                    liquidity REAL,
                    volume_1h REAL,
                    volume_24h REAL,
                    price_change_1h REAL,
                    price_change_24h REAL,
                    mint_enabled BOOLEAN,
                    freeze_enabled BOOLEAN,
                    lp_burnt_percentage REAL,
                    holders_count INTEGER,
                    top_holder_percentage REAL,
                    sniper_count INTEGER,
                    sniper_percentage REAL,
                    first_20_fresh INTEGER,
                    ath_market_cap REAL,
                    ath_multiplier REAL,
                    owner_renounced BOOLEAN,
                    telegram_url TEXT,
                    twitter_url TEXT,
                    website_url TEXT,
                    dev_sol_balance REAL,
                    dev_token_percentage REAL,
                    first_seen TIMESTAMP,
                    status TEXT,
                    sniper_wallets JSON
                )
            """)
            
                        
            # 1. Sniper wallets lentelƒó
            await self.execute("""
                CREATE TABLE IF NOT EXISTS sniper_wallets (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    address TEXT NOT NULL UNIQUE,
                    type TEXT NOT NULL,
                    success_rate REAL DEFAULT 0.0,
                    total_trades INTEGER DEFAULT 0,
                    last_active TIMESTAMP,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

            # 2. Token updates lentelƒó
            await self.execute("""
                CREATE TABLE IF NOT EXISTS token_updates (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    address TEXT NOT NULL,
                    all_metrics TEXT NOT NULL,
                    timestamp TIMESTAMP,
                    current_multiplier REAL,
                    FOREIGN KEY(address) REFERENCES token_initial_states(address)
                )
            """)

            # 3. Successful tokens lentelƒó
            await self.execute("""
                CREATE TABLE IF NOT EXISTS successful_tokens (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    address TEXT NOT NULL UNIQUE,
                    initial_parameters TEXT NOT NULL,
                    time_to_10x INTEGER,
                    discovery_timestamp TIMESTAMP,
                    FOREIGN KEY(address) REFERENCES token_initial_states(address)
                )
            """)

            # 4. Failed tokens lentelƒó
            await self.execute("""
                CREATE TABLE IF NOT EXISTS failed_tokens (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    address TEXT NOT NULL UNIQUE,
                    initial_parameters TEXT NOT NULL,
                    failure_reason TEXT,
                    discovery_timestamp TIMESTAMP,
                    FOREIGN KEY(address) REFERENCES token_initial_states(address)
                )
            """)
            
            # Indeksai
            await self.execute("CREATE INDEX IF NOT EXISTS idx_token_updates_address ON token_updates(address)")
            await self.execute("CREATE INDEX IF NOT EXISTS idx_token_updates_timestamp ON token_updates(timestamp)")
            
            self.setup_done = True
            logger.info(f"[2025-01-31 13:22:07] Database setup completed")
            
        except Exception as e:
            logger.error(f"[2025-01-31 13:22:07] Database setup error: {e}")
            raise
            
    async def execute(self, query: str, *args):
        """Vykdo SQL u≈æklausƒÖ"""
        if not self.conn:
            await self.connect()
            
        try:
            # Jei args yra tuple viename tuple, i≈°pakuojame jƒØ
            if len(args) == 1 and isinstance(args[0], tuple):
                args = args[0]
                
            async with self.conn.execute(query, args) as cursor:
                await self.conn.commit()
                return cursor.rowcount
        except Exception as e:
            logger.error(f"[2025-01-31 14:00:32] Query execution error: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Args: {args}")
            raise
            
    async def fetch_all(self, query: str, *args):
        """Gauna visus rezultatus i≈° u≈æklausos"""
        if not self.conn:
            await self.connect()
            
        try:
            async with self.conn.execute(query, args) as cursor:
                return await cursor.fetchall()
        except Exception as e:
            logger.error(f"[2025-01-31 13:22:07] Fetch all error: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Args: {args}")
            raise
            
    async def fetch_one(self, query: str, *args):
        """Gauna vienƒÖ rezultatƒÖ i≈° u≈æklausos"""
        if not self.conn:
            await self.connect()
            
        try:
            async with self.conn.execute(query, args) as cursor:
                return await cursor.fetchone()
        except Exception as e:
            logger.error(f"[2025-01-31 13:22:07] Fetch one error: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Args: {args}")
            raise
            
    async def close(self):
        """U≈ædaro duomen≈≥ bazƒós prisijungimƒÖ"""
        if self.conn:
            await self.conn.close()
            self.conn = None
            logger.info(f"[2025-01-31 13:22:07] Database connection closed")


class Config:
    # Telegram settings
    TELEGRAM_API_ID = '25425140'
    TELEGRAM_API_HASH = 'bd0054bc5393af360bc3930a27403c33'
    TELEGRAM_SOURCE_CHATS = ['@solearlytrending', '@botubotass']
    TELEGRAM_DEST_CHAT = '@smartas1'
    TELEGRAM_DEST_CHAT1 = '@testasmano'
    
    # Scanner settings
    SCANNER_GROUP = '@skaneriss'
    SOUL_SCANNER_BOT = 6872314605
    SYRAX_SCANNER_BOT = 7488438206
    
    # ML settings
    MIN_TRAINING_SAMPLES = 10
    RETRAIN_INTERVAL = 12
    
    # Database settings
    DB_PATH = 'gem_finder.db'
    
    # Gem detection settings
    MIN_GEM_PROBABILITY = 0.1
    MIN_MC_FOR_GEM = 30000
    MAX_MC_FOR_GEM = 1000000
    
    # ML modelio parametrai
    ML_SETTINGS = {
        'min_gem_multiplier': 5.0,
        'update_interval': 60,  # sekundƒós
        'confidence_threshold': 0.7,
        'training_data_limit': 1000  # kiek istorini≈≥ gem naudoti apmokymui
    }
    
    # Token stebƒójimo parametrai
    MONITORING = {
        'max_tracking_time': 7 * 24 * 60 * 60,  # 7 dienos sekundƒómis
        'update_frequency': 60,  # sekundƒós
        'max_active_tokens': 1000
    }
    
    # Duomen≈≥ bazƒós parametrai
    DB_SETTINGS = {
        'max_connections': 20,
        'timeout': 30,
        'retry_attempts': 3
    }

@dataclass
class TokenMetrics:
    # Basic info
    address: str
    name: str
    symbol: str
    age: str  # format: "1h", "2d", etc.
    
    # Price metrics
    market_cap: float
    liquidity: float
    volume_1h: float
    volume_24h: float
    price_change_1h: float
    price_change_24h: float
    ath_market_cap: float
    
    lp_burnt_percentage: float
    holders_count: int
    top_holder_percentage: float
    sniper_count: int
    sniper_percentage: float
    first_20_fresh: int
    
    

    mint_enabled: int = 0      # Vietoj bool = False
    freeze_enabled: int = 0    # Vietoj bool = False
    owner_renounced: int = 0
    total_scans: int = 0      # Vietoj bool = False

    # Nauji laukai i≈° Syrax Scanner
    dev_bought_tokens: float = 0.0,
    dev_bought_sol: float = 0.0,
    dev_created_tokens: int = 0,
    same_name_count: int = 0,
    same_website_count: int = 0,
    same_telegram_count: int = 0,
    same_twitter_count: int = 0,
    bundle_count: int = 0,
    bundle_supply_percentage: float = 0.0,
    bundle_curve_percentage: float = 0.0,
    bundle_sol: float = 0.0
    
    # Optional fields with default values
    ath_multiplier: float = 1.0
    dev_wallet: Optional[str] = None
    sniper_wallets: List[Dict] = None
    dev_sol_balance: float = 0.0
    dev_token_percentage: float = 0.0
    telegram_url: Optional[str] = None
    twitter_url: Optional[str] = None
    website_url: Optional[str] = None
    

    def to_dict(self) -> Dict:
        """Konvertuoja objektƒÖ ƒØ ≈æodynƒÖ"""
        return {
            'address': self.address,
            'name': self.name,
            'symbol': self.symbol,
            'age': self.age,
            'market_cap': self.market_cap,
            'liquidity': self.liquidity,
            'volume_1h': self.volume_1h,
            'volume_24h': self.volume_24h,
            'price_change_1h': self.price_change_1h,
            'price_change_24h': self.price_change_24h,
            'ath_market_cap': self.ath_market_cap,
            'lp_burnt_percentage': self.lp_burnt_percentage,
            'holders_count': self.holders_count,
            'top_holder_percentage': self.top_holder_percentage,
            'sniper_count': self.sniper_count,
            'sniper_percentage': self.sniper_percentage,
            'first_20_fresh': self.first_20_fresh,
            'mint_enabled': self.mint_enabled,
            'freeze_enabled': self.freeze_enabled,
            'owner_renounced': self.owner_renounced,
            'ath_multiplier': self.ath_multiplier,
            'dev_wallet': self.dev_wallet,
            'sniper_wallets': self.sniper_wallets,
            'dev_sol_balance': self.dev_sol_balance,
            'dev_token_percentage': self.dev_token_percentage,
            'telegram_url': self.telegram_url,
            'twitter_url': self.twitter_url,
            'website_url': self.website_url,
            'dev_bought_tokens': self.dev_bought_tokens,
            'dev_bought_sol': self.dev_bought_sol,
            'dev_created_tokens': self.dev_created_tokens,
            'same_name_count': self.same_name_count,
            'same_website_count': self.same_website_count,
            'same_telegram_count': self.same_telegram_count,
            'same_twitter_count': self.same_twitter_count,
            'bundle_count': self.bundle_count,
            'bundle_supply_percentage': self.bundle_supply_percentage,
            'bundle_curve_percentage': self.bundle_curve_percentage,
            'bundle_sol': self.bundle_sol
        }
    
@dataclass
class TokenUpdate:
    """Token update data structure"""
    address: str
    timestamp: datetime
    market_cap: float
    liquidity: float
    volume_1h: float
    volume_24h: float
    price_change_1h: float
    price_change_24h: float
    holders_count: int
    top_holder_percentage: float
    sniper_count: int
    sniper_percentage: float
    current_multiplier: float
    all_metrics: Dict
    
    # Pridedami tr≈´kstami laukai
    name: str
    symbol: str
    age: str
    mint_enabled: int = 0
    freeze_enabled: int = 0
    lp_burnt_percentage: float = 0.0
    first_20_fresh: int = 0
    ath_market_cap: float = 0.0
    ath_multiplier: float = 1.0
    owner_renounced: int = 0
    telegram_url: Optional[str] = None
    twitter_url: Optional[str] = None
    website_url: Optional[str] = None
    dev_sol_balance: float = 0.0
    dev_token_percentage: float = 0.0
    sniper_wallets: Optional[List[Dict]] = None
    has_24h_data: bool = False

    # Nauji Syrax Scanner laukai
    dev_bought_tokens: float = 0.0
    dev_bought_sol: float = 0.0
    dev_created_tokens: int = 0
    same_name_count: int = 0
    same_website_count: int = 0
    same_telegram_count: int = 0
    same_twitter_count: int = 0
    bundle_count: int = 0
    bundle_supply_percentage: float = 0.0
    bundle_curve_percentage: float = 0.0
    bundle_sol: float = 0.0
    
    def to_dict(self) -> dict:
        """Konvertuoja ƒØ ≈æodynƒÖ"""
        return self.__dict__

    def to_tuple(self) -> tuple:
        """Konvertuoja ƒØ tuple DB ƒØra≈°ymui"""
        return (
            self.address, self.name, self.symbol, self.age,
            self.market_cap, self.liquidity, self.volume_1h, self.volume_24h,
            self.price_change_1h, self.price_change_24h,
            self.mint_enabled, self.freeze_enabled, self.lp_burnt_percentage,
            self.holders_count, self.top_holder_percentage,
            self.sniper_count, self.sniper_percentage, self.first_20_fresh,
            self.ath_market_cap, self.ath_multiplier,
            self.owner_renounced, self.telegram_url, self.twitter_url, self.website_url,
            self.dev_sol_balance, self.dev_token_percentage,
            self.has_24h_data,
            json.dumps(self.sniper_wallets) if self.sniper_wallets else None,
            # Nauji Syrax Scanner laukai
            self.dev_bought_tokens, self.dev_bought_sol, self.dev_created_tokens,
            self.same_name_count, self.same_website_count, self.same_telegram_count,
            self.same_twitter_count, self.bundle_count, self.bundle_supply_percentage,
            self.bundle_curve_percentage, self.bundle_sol
            
        )
    def __post_init__(self):
        """Nustatome has_24h_data po inicializacijos"""
        self.has_24h_data = (self.volume_24h > 0 or self.price_change_24h != 0)

    
class TokenHandler:
    def __init__(self, db_manager, ml_analyzer):
        self.db = db_manager
        self.ml = ml_analyzer
        self.telegram_client = None
        
    async def handle_new_token(self, token_data: TokenMetrics):
        """Apdoroja naujƒÖ token'ƒÖ"""
        try:
            # I≈°saugome pradinƒô b≈´senƒÖ
            await self.db.save_initial_state(token_data)
            logger.info(f"[2025-02-03 14:44:23] Saved initial state for: {token_data.address}")
            
            # Gauname ML predikcijas tik vienƒÖ kartƒÖ
            initial_prediction = await self.ml.predict_potential(token_data)
            logger.info(f"[2025-02-03 14:44:23] Initial ML prediction for {token_data.address}: {initial_prediction:.2f}")
            
            # Tikriname potencialƒÖ su jau turima predikcija
            if self._meets_basic_criteria(token_data) and initial_prediction >= Config.MIN_GEM_PROBABILITY:
                await self._send_gem_notification(token_data, initial_prediction)
            
            return initial_prediction
            
        except Exception as e:
            logger.error(f"[2025-02-03 14:44:23] Error handling new token: {e}")
            return 0.0

    async def handle_token_update(self, token_address: str, new_data: TokenMetrics, is_new_token: bool = False):
        """Apdoroja token'o atnaujinimƒÖ"""
        try:
            initial_data = await self.db.get_initial_state(token_address)
            if not initial_data:
                if is_new_token:  # Jei tai naujas token'as
                    await self.handle_new_token(new_data)  # Naudojame handle_new_token metodƒÖ
                    logger.info(f"[2025-02-03 14:58:13] New token processed via update: {token_address}")
                    return
                else:
                    logger.warning(f"[2025-02-03 14:58:13] Skipping update for unknown token: {token_address}")
                    return

            current_multiplier = new_data.market_cap / initial_data.market_cap
            
            # I≈°saugome atnaujinimƒÖ
            await self.db.save_token_update(token_address, new_data)
            logger.info(f"[2025-02-03 14:58:13] Token update saved for {token_address}, current multiplier: {current_multiplier:.2f}x")
            
            # Tikriname ar tapo gem (10x)
            if current_multiplier >= 10 and not await self.db.is_gem(token_address):
                logger.info(f"[2025-02-03 14:58:13] Token reached 10x: {token_address} ({current_multiplier:.2f}X)")
                
                try:
                                        
                    # Tada ≈æymime kaip gem ir atnaujiname modelƒØ
                    await self.db.mark_as_gem(token_address)
                    await self.ml.update_model_with_new_gem(initial_data)
                    logger.info(f"[2025-02-03 14:58:13] Token marked as gem and model updated: {token_address}")
                except Exception as notification_error:
                    logger.error(f"[2025-02-03 14:58:13] Error in gem notification process: {notification_error}")
                    import traceback
                    logger.error(f"Notification error traceback: {traceback.format_exc()}")
            
            

        except Exception as e:
            logger.error(f"[2025-02-03 14:58:13] Error handling token update: {e}")
            import traceback
            logger.error(f"Main update error traceback: {traceback.format_exc()}")
            
    def _meets_basic_criteria(self, token_data: TokenMetrics) -> bool:
        """Patikriname ar token'as atitinka basic kriterijus"""
        try:
            logger.info(f"[2025-02-03 14:44:23] Checking criteria for {token_data.address}:")
            logger.info(f"Market Cap: ${token_data.market_cap:,.0f}")
            logger.info(f"Min MC: ${Config.MIN_MC_FOR_GEM:,.0f}")
            logger.info(f"Max MC: ${Config.MAX_MC_FOR_GEM:,.0f}")
            
            if token_data.market_cap < Config.MIN_MC_FOR_GEM:
                logger.info(f"[2025-02-03 14:44:23] Token {token_data.address} MC too low")
                return False
                
            if token_data.market_cap > Config.MAX_MC_FOR_GEM:
                logger.info(f"[2025-02-03 14:44:23] Token {token_data.address} MC too high")
                return False
                
            return True
            
        except Exception as e:
            logger.error(f"[2025-02-03 14:44:23] Error in basic criteria check: {e}")
            return False

    async def _send_gem_notification(self, token_data: TokenMetrics, probability: float) -> None:
        """Siunƒçia prane≈°imƒÖ apie potencial≈≥ gem"""
        try:
            gmgn_url = f"https://gmgn.ai/sol/token/{token_data.address}"
            current_time = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
            
            message = (
                f"üîç POTENTIAL GEM DETECTED!\n\n"
                f"Token: {token_data.name} (${token_data.symbol})\n"  # Naudojame token_data
                f"Address: <code>{token_data.address}</code>\n"  # Naudojame token_data
                f"Probability: {probability*100:.1f}%\n\n"
                f"Market Cap: ${token_data.market_cap:,.0f}\n"
                f"Liquidity: ${token_data.liquidity:,.0f}\n"
                f"Age: {token_data.age}\n"
                f"Holders: {token_data.holders_count}\n"
                f"Volume 1h: ${token_data.volume_1h:,.0f}\n\n"
                f"Security:\n"
                f"‚Ä¢ Mint: {'‚úÖ' if not token_data.mint_enabled else '‚ö†Ô∏è'}\n"
                f"‚Ä¢ Freeze: {'‚úÖ' if not token_data.freeze_enabled else '‚ö†Ô∏è'}\n"
                f"‚Ä¢ Owner: {'‚úÖ' if token_data.owner_renounced else '‚ö†Ô∏è'}\n\n"
                f"Links:\n"
                f"üîó <a href='{gmgn_url}'>View on GMGN.AI</a>\n"
                f"----------------------------------------------\n"
                f"üåê Website: {token_data.website_url or 'N/A'}\n"
                f"üê¶ Twitter: {token_data.twitter_url or 'N/A'}\n"
                f"üí¨ Telegram: {token_data.telegram_url or 'N/A'}\n\n"
                f"‚è∞ Found at: {current_time} UTC\n"
                f"<i>Tap token address to copy</i>"
            )
            
            await self._send_notification(message, parse_mode='HTML')
            
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error sending gem notification: {e}")

    # TokenHandler klasƒóje:
    async def check_syrax_metrics(self, token_address: str, syrax_data: Dict) -> None:
        """Tikrina Syrax Scanner metrikas ir siunƒçia ƒØspƒójimus jei reikia"""
        try:
            if not self.telegram_client:
                logger.error(f"[2025-02-09 10:49:01] Telegram client not initialized")
                return
            gmgn_url = f"https://gmgn.ai/sol/token/{token_address}"

            # Tikriname visas metrikas i≈° karto
            name_count = int(syrax_data.get('same_name_count', 0))
            website_count = int(syrax_data.get('same_website_count', 0))
            telegram_count = int(syrax_data.get('same_telegram_count', 0))
            twitter_count = int(syrax_data.get('same_twitter_count', 0))
            dev_tokens = int(syrax_data.get('dev_created_tokens', 0))
            
                        
            logger.debug(f"[2025-02-09 10:49:01] All metrics - name: {name_count}, website: {website_count}, telegram: {telegram_count}, twitter: {twitter_count}, dev_tokens: {dev_tokens}, bundle: {bundle_count}/{bundle_supply}%/{bundle_curve}%/{bundle_sol}SOL, dev_bought: {dev_bought_tokens}/{dev_bought_sol}/{dev_bought_percentage}%/{dev_bought_curve}%")
            
            warnings = []
            
            # Tikriname ar metrikos yra ma≈æesnƒós u≈æ limitƒÖ
            if name_count < 500:
                warnings.append(f"üîç Same name count: {name_count}")
            if website_count < 500:
                warnings.append(f"üåê Same website count: {website_count}")
            if telegram_count < 500:
                warnings.append(f"üì± Same telegram count: {telegram_count}")
            if twitter_count < 500:
                warnings.append(f"üê¶ Same twitter count: {twitter_count}")
                
            
            
            # Jei yra ƒØspƒójim≈≥, siunƒçiame ≈æinutƒô
            if warnings:
                message = (
                    f"‚ö†Ô∏è METRICS ALERT!\\n\\n"
                    f"Token Address: <code>{token_address}</code>\\n\\n"
                    f"Links:\\n"
                    f"üîó <a href='{gmgn_url}'>View on GMGN.AI</a>\\n\\n"
                    f"Metrics:\\n"
                    f"{chr(10).join(warnings)}\\n\\n"
                    f"<i>Tap token address to copy</i>"
                )
                
                await self._send_notification(message, parse_mode='HTML')
                logger.info(f"[2025-02-09 10:49:01] Sent metrics alert for {token_address}")
                    
        except Exception as e:
            logger.error(f"[2025-02-09 10:49:01] Error checking Syrax metrics: {str(e)}")

   

    async def _send_notification(self, message: str, parse_mode: str = None) -> None:
        """Siunƒçia prane≈°imƒÖ ƒØ Telegram"""
        try:
            if not self.telegram_client:
                logger.error(f"[{datetime.now(timezone.utc)}] Telegram client not initialized")
                return
                
            await self.telegram_client.send_message(Config.TELEGRAM_DEST_CHAT1, message, parse_mode=parse_mode)
            logger.info(f"[{datetime.now(timezone.utc)}] Sent notification to Telegram")
            
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error sending notification: {e}")
            logger.error(f"[{datetime.now(timezone.utc)}] Error details: {str(e)}")

    async def check_inactive_tokens(self):
        """Tikrina ir pa≈æymi neaktyvius token'us"""
        try:
            query = "SELECT address, first_seen FROM token_initial_states WHERE status = 'new'"
            tokens = await self.db.fetch_all(query)
            
            current_time = datetime.now(timezone.utc)
            checked_count = 0
            failed_count = 0
            
            for token in tokens:
                try:
                    first_seen = datetime.fromisoformat(str(token['first_seen']))
                    time_difference = current_time - first_seen
                    
                    if time_difference.total_seconds() > 6 * 3600:
                        updates_query = "SELECT COUNT(*) as count FROM token_updates WHERE address = ?"
                        update_count = await self.db.fetch_one(updates_query, (token['address'],))
                        
                        if update_count['count'] == 0:
                            await self.db.mark_as_failed(
                                token['address'],
                                f"No updates for {time_difference.total_seconds() / 3600:.1f} hours"
                            )
                            failed_count += 1
                            logger.info(f"[2025-02-03 14:44:23] Marked {token['address']} as failed - no updates")
                    
                    checked_count += 1
                    
                except Exception as e:
                    logger.error(f"[2025-02-03 14:44:23] Error checking token {token['address']}: {e}")
                    continue
            
            logger.info(f"[2025-02-03 14:44:23] Inactive tokens check completed - Checked: {checked_count}, Failed: {failed_count}")
            
        except Exception as e:
            logger.error(f"[2025-02-03 14:44:23] Error in check_inactive_tokens: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

class MLAnalyzer:
    def __init__(self, db_manager, token_analyzer=None):
        self.db = db_manager
        self.token_analyzer = token_analyzer
        self.model = None
        self.scaler = StandardScaler()
        
        # Performance tracking
        self.predictions_made = 0
        self.successful_predictions = 0
        self.prediction_history = []  # Saugosime predikcij≈≥ istorijƒÖ
        
        if self.token_analyzer is None:
            self.token_analyzer = TokenAnalyzer(db_manager, self)

    async def train_model(self):
        """Treniruoja modelƒØ naudojant VISUS pradinius token'≈≥ duomenis"""
        try:
            successful_tokens = await self.db.get_all_gems()
            failed_tokens = await self.db.get_failed_tokens()
            
            logger.info(f"[2025-02-03 13:21:21] Starting model training with {len(successful_tokens)} gems and {len(failed_tokens)} failed tokens")
            
            def process_features(token_metrics, features):
                """Helper funkcija feature vektoriaus formavimui"""
                try:
                    feature_vector = []
                    
                    # Basic metrics
                    feature_vector.extend([
                        float(features['basic_metrics']['age_hours']),
                        float(features['basic_metrics']['market_cap_normalized']),
                        float(features['basic_metrics']['liquidity_ratio'])
                    ])
                    
                    # Price/Volume metrics
                    feature_vector.extend([
                        float(token_metrics.market_cap or 0),
                        float(token_metrics.liquidity or 0),
                        float(token_metrics.volume_1h or 0),
                        float(token_metrics.volume_24h or 0),
                        float(token_metrics.price_change_1h or 0),
                        float(token_metrics.price_change_24h or 0)
                    ])
                    
                    # Holder metrics
                    feature_vector.extend([
                        float(features['holder_metrics']['holder_distribution']),
                        float(features['holder_metrics']['value_per_holder']),
                        float(features['holder_metrics']['top_holder_risk'])
                    ])
                    
                    # Sniper metrics
                    feature_vector.extend([
                        float(features['sniper_metrics'].get('sniper_impact', 0)),
                        float(features['sniper_metrics'].get('whale_dominance', 0)),
                        float(features['sniper_metrics'].get('fish_ratio', 0)),
                        float(features['sniper_metrics'].get('sniper_behavior_pattern', 0))
                    ])
                    
                    # Dev metrics
                    feature_vector.extend([
                        float(features['dev_metrics']['dev_commitment']),
                        float(features['dev_metrics']['owner_risk']),
                        float(features['dev_metrics']['dev_sol_strength']),
                        float(features['dev_metrics']['dev_token_risk']),
                        float(features['dev_metrics']['ownership_score'])
                    ])
                    
                    # Security metrics
                    feature_vector.extend([
                        float(features['security_metrics']['contract_security']),
                        float(features['security_metrics']['lp_security']),
                        float(features['security_metrics']['mint_risk']),
                        float(features['security_metrics']['freeze_risk']),
                        float(features['security_metrics']['overall_security_score'])
                    ])

                    # Scan metrics
                    feature_vector.extend([
                        float(features['scan_metrics']['scan_count']),
                        float(features['scan_metrics']['scan_intensity']),
                        float(features['scan_metrics']['scan_momentum'])
                    ])
                    
                    # Social metrics
                    feature_vector.extend([
                        float(features['social_metrics']['social_presence']),
                        float(features['social_metrics']['has_twitter']),
                        float(features['social_metrics']['has_website']),
                        float(features['social_metrics']['has_telegram']),
                        float(features['social_metrics']['social_risk'])
                    ])
                    
                    # Risk assessment
                    feature_vector.extend([
                        float(features['risk_assessment']['overall_risk']),
                        float(features['risk_assessment']['pump_dump_risk']),
                        float(features['risk_assessment']['security_risk']),
                        float(features['risk_assessment']['holder_risk']),
                        float(features['risk_assessment']['dev_risk'])
                    ])
                    
                    # Contract flags
                    feature_vector.extend([
                        float(token_metrics.mint_enabled),
                        float(token_metrics.freeze_enabled),
                        float(token_metrics.owner_renounced)
                    ])
                    
                    return feature_vector
                except Exception as e:
                    logger.error(f"[2025-02-03 13:21:21] Error processing features: {e}")
                    return None

            # Process successful tokens
            X_success = []
            y_success = []
            logger.info(f"[2025-02-03 13:21:21] Processing successful tokens...")
            
            for token in successful_tokens:
                try:
                    initial_state = json.loads(token['initial_parameters'])
                    token_metrics = TokenMetrics(**initial_state)
                    features = self.token_analyzer.prepare_features(token_metrics, [])
                    
                    if not features:
                        continue
                        
                    feature_vector = process_features(token_metrics, features)
                    if feature_vector:
                        X_success.append(feature_vector)
                        y_success.append(1)  # 1 = success
                        logger.info(f"[2025-02-03 13:21:21] Processed gem {token['address']} - Features: {len(feature_vector)}")
                        
                except Exception as e:
                    logger.error(f"[2025-02-03 13:21:21] Error processing gem {token['address']}: {e}")
                    continue

            # Process failed tokens
            X_failed = []
            y_failed = []
            logger.info(f"[2025-02-03 13:21:21] Processing failed tokens...")
            
            for token in failed_tokens:
                try:
                    initial_state = json.loads(token['initial_parameters'])
                    token_metrics = TokenMetrics(**initial_state)
                    features = self.token_analyzer.prepare_features(token_metrics, [])
                    
                    if not features:
                        continue
                        
                    feature_vector = process_features(token_metrics, features)
                    if feature_vector:
                        X_failed.append(feature_vector)
                        y_failed.append(0)  # 0 = failed
                        
                except Exception as e:
                    logger.error(f"[2025-02-03 13:21:21] Error processing failed token {token['address']}: {e}")
                    continue

            # Combine and normalize data
            if not X_success or not X_failed:
                logger.error("[2025-02-03 13:21:21] No valid training data")
                return
                
            X = np.vstack([X_success, X_failed])
            y = np.hstack([y_success, y_failed])
            
            # Normalize features
            X_scaled = self.scaler.fit_transform(X)
            
            # Train model
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=4,
                min_samples_leaf=2,
                random_state=42
            )
            
            self.model.fit(X_scaled, y)
            
            # Analyze feature importance
            feature_names = self._get_all_feature_names()
            importances = self.model.feature_importances_
            
            logger.info("\n=== FEATURE IMPORTANCE ANALYSIS ===")
            feature_importance = list(zip(feature_names, importances))
            feature_importance.sort(key=lambda x: x[1], reverse=True)
            
            logger.info("Top 20 Most Important Features:")
            for name, importance in feature_importance[:20]:
                logger.info(f"{name}: {importance*100:.2f}%")
            
            # Calculate training accuracy
            train_accuracy = self.model.score(X_scaled, y)
            logger.info(f"[2025-02-03 13:21:21] Training Accuracy: {train_accuracy*100:.2f}%")
            logger.info(f"[2025-02-03 13:21:21] Model successfully trained with {len(X)} samples")
            
        except Exception as e:
            logger.error(f"[2025-02-03 13:21:21] Error training model: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    async def predict_potential(self, token_data: TokenMetrics) -> float:
        """Prognozuoja token'o potencialƒÖ tapti gemu"""
        try:
            if token_data is None:
                logger.error("[2025-02-03 16:31:24] Token data is None")
                return 0.0
                
            if not self.model:
                logger.info("[2025-02-03 16:31:24] Model not found, training new model...")
                await self.train_model()
                if not self.model:
                    logger.error("[2025-02-03 16:31:24] Failed to train model")
                    return 0.0

            # Analizuojame token'ƒÖ
            features = self.token_analyzer.prepare_features(token_data, [])
            if not features:
                logger.error(f"[2025-02-03 16:31:24] Failed to analyze token {token_data.address}")
                return 0.0
                
            # Naudojame tƒÖ patƒØ feature vektori≈≥ formavimo b≈´dƒÖ kaip train_model metode
            feature_vector = []
            
            # Basic metrics
            feature_vector.extend([
                float(features['basic_metrics']['age_hours']),
                float(features['basic_metrics']['market_cap_normalized']),
                float(features['basic_metrics']['liquidity_ratio'])
            ])
            
            # Price/Volume metrics
            feature_vector.extend([
                float(token_data.market_cap or 0),
                float(token_data.liquidity or 0),
                float(token_data.volume_1h or 0),
                float(token_data.volume_24h or 0),
                float(token_data.price_change_1h or 0),
                float(token_data.price_change_24h or 0)
            ])
            
            # Holder metrics
            feature_vector.extend([
                float(features['holder_metrics']['holder_distribution']),
                float(features['holder_metrics']['value_per_holder']),
                float(features['holder_metrics']['top_holder_risk'])
            ])
            
            # Sniper metrics
            feature_vector.extend([
                float(features['sniper_metrics'].get('sniper_impact', 0)),
                float(features['sniper_metrics'].get('whale_dominance', 0)),
                float(features['sniper_metrics'].get('fish_ratio', 0)),
                float(features['sniper_metrics'].get('sniper_behavior_pattern', 0))
            ])
            
            # Dev metrics
            feature_vector.extend([
                float(features['dev_metrics']['dev_commitment']),
                float(features['dev_metrics']['owner_risk']),
                float(features['dev_metrics']['dev_sol_strength']),
                float(features['dev_metrics']['dev_token_risk']),
                float(features['dev_metrics']['ownership_score'])
            ])
            
            # Security metrics
            feature_vector.extend([
                float(features['security_metrics']['contract_security']),
                float(features['security_metrics']['lp_security']),
                float(features['security_metrics']['mint_risk']),
                float(features['security_metrics']['freeze_risk']),
                float(features['security_metrics']['overall_security_score'])
            ])

            # Scan metrics
            feature_vector.extend([
                float(features['scan_metrics']['scan_count']),
                float(features['scan_metrics']['scan_intensity']),
                float(features['scan_metrics']['scan_momentum'])
            ])
            
            # Social metrics
            feature_vector.extend([
                float(features['social_metrics']['social_presence']),
                float(features['social_metrics']['has_twitter']),
                float(features['social_metrics']['has_website']),
                float(features['social_metrics']['has_telegram']),
                float(features['social_metrics']['social_risk'])
            ])
            
            # Risk assessment
            feature_vector.extend([
                float(features['risk_assessment']['overall_risk']),
                float(features['risk_assessment']['pump_dump_risk']),
                float(features['risk_assessment']['security_risk']),
                float(features['risk_assessment']['holder_risk']),
                float(features['risk_assessment']['dev_risk'])
            ])
            
            # Contract flags
            feature_vector.extend([
                float(token_data.mint_enabled),
                float(token_data.freeze_enabled),
                float(token_data.owner_renounced)
            ])

            X = np.array(feature_vector).reshape(1, -1)
            X_scaled = self.scaler.transform(X)
            
            if np.isnan(X_scaled).any():
                logger.error("[2025-02-03 16:31:24] NaN values in prediction features")
                return 0.0
                
            probability = self.model.predict_proba(X_scaled)[0][1]
            return probability
                
        except Exception as e:
            logger.error(f"[2025-02-03 16:31:24] Error predicting potential: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return 0.0

    def _log_detailed_analysis(self, token_data: TokenMetrics, features: Dict, probability: float, X_scaled: np.array):
        """Logina i≈°samiƒÖ token'o analizƒô"""
        try:
            logger.info(f"\n=== TOKEN PREDICTION ANALYSIS ===")
            logger.info(f"Token: {token_data.symbol} ({token_data.address})")
            logger.info(f"Analysis Time: {datetime.now(timezone.utc)}")
            logger.info(f"Gem Probability: {probability*100:.2f}%")
            
            # Bazinƒó informacija
            logger.info("\nüîç BASIC METRICS")
            logger.info(f"Age: {token_data.age}")
            logger.info(f"Market Cap: ${token_data.market_cap:,.0f}")
            logger.info(f"Liquidity: ${token_data.liquidity:,.0f}")
            logger.info(f"Volume 1h: ${token_data.volume_1h:,.0f}")
            
            # Holders & Snipers
            logger.info("\nüë• HOLDERS & SNIPERS")
            logger.info(f"Total Holders: {token_data.holders_count}")
            logger.info(f"Top Holder %: {token_data.top_holder_percentage:.1f}%")
            logger.info(f"Sniper Count: {token_data.sniper_count}")
            
            # Security Metrics
            logger.info("\nüîí SECURITY STATUS")
            logger.info(f"‚Ä¢ Contract Security: {features['security_metrics']['contract_security']:.2f}/1.0")
            logger.info(f"‚Ä¢ LP Security: {features['security_metrics']['lp_security']:.2f}/1.0")
            logger.info(f"‚Ä¢ Overall Security: {features['security_metrics']['overall_security_score']:.2f}/1.0")
            
            # Risk Assessment
            logger.info("\n‚ö†Ô∏è RISK ASSESSMENT")
            for risk_type, risk_value in features['risk_assessment'].items():
                risk_level = "LOW" if risk_value < 0.3 else "MEDIUM" if risk_value < 0.7 else "HIGH"
                logger.info(f"‚Ä¢ {risk_type}: {risk_value:.2f} ({risk_level})")
            
            # Feature Impact Analysis
            self._log_feature_impact(X_scaled[0])
            
            # Final Decision
            self._log_decision_summary(probability, features)
            
        except Exception as e:
            logger.error(f"[2025-02-03 12:44:18] Error in detailed analysis: {e}")

    def _log_feature_impact(self, feature_values: np.array):
        """Logina feature'≈≥ ƒØtakƒÖ sprendimui"""
        try:
            feature_names = self._get_all_feature_names()
            importances = self.model.feature_importances_
            
            impacts = []
            for name, value, importance in zip(feature_names, feature_values, importances):
                impact = abs(value * importance)
                impacts.append((name, value, importance, impact))
            
            impacts.sort(key=lambda x: x[3], reverse=True)
            
            logger.info("\nüìä FEATURE IMPACT ANALYSIS")
            logger.info("Top 5 Influencing Factors:")
            for name, value, importance, impact in impacts[:5]:
                direction = "POSITIVE" if value > 0 else "NEGATIVE"
                logger.info(f"‚Ä¢ {name}:")
                logger.info(f"  - Impact: {impact:.3f} ({direction})")
                logger.info(f"  - Importance: {importance*100:.1f}%")
                
        except Exception as e:
            logger.error(f"[2025-02-03 12:44:18] Error in feature impact analysis: {e}")

    def _log_decision_summary(self, probability: float, features: Dict):
        """Logina galutinƒØ sprendimƒÖ"""
        try:
            logger.info("\nüéØ FINAL ASSESSMENT")
            if probability >= Config.MIN_GEM_PROBABILITY:
                logger.info("‚úÖ HIGH POTENTIAL GEM")
                positive_factors = []
                for metric_type, metrics in features.items():
                    for name, value in metrics.items():
                        if isinstance(value, (int, float)) and value > 0.7:
                            positive_factors.append(f"{name} ({value:.2f})")
                logger.info(f"Main positive factors: {', '.join(positive_factors[:3])}")
            else:
                logger.info("‚ùå LOW POTENTIAL")
                concerns = []
                for metric_type, metrics in features.items():
                    for name, value in metrics.items():
                        if isinstance(value, (int, float)) and value < 0.3:
                            concerns.append(f"{name} ({value:.2f})")
                logger.info(f"Main concerns: {', '.join(concerns[:3])}")
                
        except Exception as e:
            logger.error(f"[2025-02-03 12:44:18] Error in decision summary: {e}")

    async def update_model_with_new_gem(self, token_data: TokenMetrics):
        """Atnaujina modelƒØ su nauju gem"""
        try:
            if token_data is None:
                logger.error(f"[2025-02-03 12:44:18] Cannot update model - token data is None")
                return

            # Atnaujiname statistikƒÖ
            self.successful_predictions += 1
            
            # Retrainuojame modelƒØ su naujais duomenimis
            await self.train_model()
            
            # Loginame accuracy
            if self.predictions_made > 0:
                accuracy = (self.successful_predictions / self.predictions_made) * 100
                logger.info(f"[2025-02-03 12:44:18] Current model accuracy: {accuracy:.2f}%")
                logger.info(f"[2025-02-03 12:44:18] Total predictions: {self.predictions_made}")
                logger.info(f"[2025-02-03 12:44:18] Successful predictions: {self.successful_predictions}")
            
            
            
        except Exception as e:
            logger.error(f"[2025-02-03 12:44:18] Error updating model: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    def _get_all_feature_names(self) -> List[str]:
        """GrƒÖ≈æina VIS≈≤ features pavadinimus"""
        feature_names = [
            # Basic Metrics
            'age_hours',
            'market_cap_normalized',
            'liquidity_ratio',
            'market_cap',
            'liquidity', 
            'volume_1h',
            'volume_24h',
            'price_change_1h',
            'price_change_24h',
            'has_24h_data',

            # Holder Metrics
            'holder_distribution',
            'value_per_holder',
            'holder_retention', 
            'top_holder_risk',

            # Sniper Metrics
            'sniper_impact',
            'whale_dominance',
            'fish_ratio',
            'sniper_diversity',
            'sniper_behavior_pattern',

            # Dev Metrics
            'dev_commitment',
            'owner_risk',
            'dev_sol_strength',
            'dev_token_risk',
            'ownership_score',

            # Security Metrics 
            'contract_security',
            'lp_security',
            'mint_risk',
            'freeze_risk',
            'overall_security_score',

            # Social Metrics
            'social_presence',
            'has_twitter',
            'has_website',
            'has_telegram',
            'social_risk',

            # Scan Metrics
            'scan_count',
            'scan_intensity',
            'scan_momentum',

            # Risk Assessment
            'overall_risk',
            'pump_dump_risk', 
            'security_risk',
            'holder_risk',
            'dev_risk',

            # Contract Flags
            'mint_enabled',
            'freeze_enabled',
            'owner_renounced'
        ]

        # Debug info
        logger.info(f"\n=== FEATURE LIST ({len(feature_names)} total) ===")
        for i, feature in enumerate(feature_names):
            logger.info(f"{i+1}. {feature}")

        return feature_names

async def analyze_success_patterns(self):
    """Analizuoja sƒókming≈≥ gem≈≥ ≈°ablonus ir laiko faktorius"""
    try:
        logger.info("[2025-02-03 12:46:07] Starting success pattern analysis...")
        successful_tokens = await self.db.get_all_gems()
        
        # Grupuojame pagal laikƒÖ iki 10x
        time_groups = {
            'quick': [],    # < 1 valanda
            'medium': [],   # 1-6 valandos
            'slow': []      # > 6 valandos
        }
        
        patterns = {
            'quick': {},
            'medium': {},
            'slow': {}
        }
        
        for token in successful_tokens:
            try:
                time_to_10x = token['time_to_10x']
                initial_state = json.loads(token['initial_parameters'])
                token_metrics = TokenMetrics(**initial_state)
                
                # Nustatome grupƒô
                if time_to_10x < 3600:  # < 1h
                    group = 'quick'
                elif time_to_10x < 21600:  # < 6h
                    group = 'medium'
                else:
                    group = 'slow'
                    
                time_groups[group].append(token)
                
                # Analizuojame kiekvienos grupƒós patterns
                features = self.token_analyzer.prepare_features(token_metrics, [])
                if not features:
                    continue
                    
                if group not in patterns:
                    patterns[group] = {
                        'market_caps': [],
                        'liquidities': [],
                        'holder_counts': [],
                        'sniper_counts': [],
                        'security_scores': [],
                        'social_scores': [],
                        'risk_scores': []
                    }
                    
                patterns[group]['market_caps'].append(token_metrics.market_cap)
                patterns[group]['liquidities'].append(token_metrics.liquidity)
                patterns[group]['holder_counts'].append(token_metrics.holders_count)
                patterns[group]['sniper_counts'].append(token_metrics.sniper_count)
                patterns[group]['security_scores'].append(features['security_metrics']['overall_security_score'])
                patterns[group]['social_scores'].append(features['social_metrics']['social_presence'])
                patterns[group]['risk_scores'].append(features['risk_assessment']['overall_risk'])
                
            except Exception as e:
                logger.error(f"[2025-02-03 12:46:07] Error processing token {token['address']}: {e}")
                continue
        
        logger.info("\n=== GEM SUCCESS PATTERN ANALYSIS ===")
        
        for group in ['quick', 'medium', 'slow']:
            count = len(time_groups[group])
            if count == 0:
                continue
                
            logger.info(f"\nüîç {group.upper()} GEMS (Time to 10x: {'<1h' if group == 'quick' else '1-6h' if group == 'medium' else '>6h'})")
            logger.info(f"Total Gems in Group: {count}")
            
            p = patterns[group]
            logger.info("\nTypical Characteristics:")
            logger.info(f"‚Ä¢ Market Cap: ${np.median(p['market_caps']):,.0f} (median)")
            logger.info(f"‚Ä¢ Liquidity: ${np.median(p['liquidities']):,.0f} (median)")
            logger.info(f"‚Ä¢ Holders: {np.median(p['holder_counts']):.0f} (median)")
            logger.info(f"‚Ä¢ Snipers: {np.median(p['sniper_counts']):.0f} (median)")
            logger.info(f"‚Ä¢ Security Score: {np.mean(p['security_scores']):.2f}/1.0")
            logger.info(f"‚Ä¢ Social Score: {np.mean(p['social_scores']):.2f}/1.0")
            logger.info(f"‚Ä¢ Risk Score: {np.mean(p['risk_scores']):.2f}/1.0")
            
        return patterns
        
    except Exception as e:
        logger.error(f"[2025-02-03 12:46:07] Error analyzing success patterns: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return None

async def analyze_trend_changes(self):
    """Analizuoja kaip keiƒçiasi gem≈≥ ≈°ablonai per laikƒÖ"""
    try:
        logger.info("[2025-02-03 12:46:07] Starting trend change analysis...")
        successful_tokens = await self.db.get_all_gems()
        
        # Grupuojame pagal mƒónesius
        monthly_patterns = {}
        
        for token in successful_tokens:
            try:
                discovery_time = datetime.fromisoformat(str(token['discovery_timestamp']))
                month_key = f"{discovery_time.year}-{discovery_time.month:02d}"
                
                initial_state = json.loads(token['initial_parameters'])
                token_metrics = TokenMetrics(**initial_state)
                features = self.token_analyzer.prepare_features(token_metrics, [])
                
                if not features:
                    continue
                    
                if month_key not in monthly_patterns:
                    monthly_patterns[month_key] = {
                        'count': 0,
                        'avg_market_cap': 0,
                        'avg_liquidity': 0,
                        'avg_holders': 0,
                        'avg_security_score': 0,
                        'avg_social_score': 0,
                        'avg_risk_score': 0,
                        'success_time': []
                    }
                
                p = monthly_patterns[month_key]
                p['count'] += 1
                p['avg_market_cap'] += token_metrics.market_cap
                p['avg_liquidity'] += token_metrics.liquidity
                p['avg_holders'] += token_metrics.holders_count
                p['avg_security_score'] += features['security_metrics']['overall_security_score']
                p['avg_social_score'] += features['social_metrics']['social_presence']
                p['avg_risk_score'] += features['risk_assessment']['overall_risk']
                p['success_time'].append(token['time_to_10x'])
                
            except Exception as e:
                logger.error(f"[2025-02-03 12:46:07] Error processing token {token['address']}: {e}")
                continue
        
        # Skaiƒçiuojame vidurkius ir analizuojame trendus
        logger.info("\n=== GEM TREND ANALYSIS OVER TIME ===")
        
        sorted_months = sorted(monthly_patterns.keys())
        for month in sorted_months:
            p = monthly_patterns[month]
            count = p['count']
            if count == 0:
                continue
                
            # Skaiƒçiuojame vidurkius
            for key in ['avg_market_cap', 'avg_liquidity', 'avg_holders', 'avg_security_score', 
                       'avg_social_score', 'avg_risk_score']:
                p[key] /= count
                
            logger.info(f"\nüìÖ {month} Analysis (Total Gems: {count})")
            logger.info(f"‚Ä¢ Average Market Cap: ${p['avg_market_cap']:,.0f}")
            logger.info(f"‚Ä¢ Average Liquidity: ${p['avg_liquidity']:,.0f}")
            logger.info(f"‚Ä¢ Average Holders: {p['avg_holders']:.0f}")
            logger.info(f"‚Ä¢ Average Security Score: {p['avg_security_score']:.2f}/1.0")
            logger.info(f"‚Ä¢ Average Social Score: {p['avg_social_score']:.2f}/1.0")
            logger.info(f"‚Ä¢ Average Risk Score: {p['avg_risk_score']:.2f}/1.0")
            logger.info(f"‚Ä¢ Median Time to 10x: {np.median(p['success_time'])/3600:.1f}h")
        
        return monthly_patterns
        
    except Exception as e:
        logger.error(f"[2025-02-03 12:46:07] Error analyzing trend changes: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        return None

async def analyze_feature_correlations(self):
    """Analizuoja feature'≈≥ tarpusavio koreliacijas ir j≈≥ ƒØtakƒÖ gem statusui"""
    try:
        logger.info(f"[2025-02-03 12:46:07] Starting feature correlation analysis...")
        
        successful_tokens = await self.db.get_all_gems()
        failed_tokens = await self.db.get_failed_tokens()
        
        all_features = []
        all_labels = []
        
        # Renkame features i≈° sƒókming≈≥ ir nesƒókming≈≥ token≈≥
        for token in successful_tokens + failed_tokens:
            try:
                initial_state = json.loads(token['initial_parameters'])
                token_metrics = TokenMetrics(**initial_state)
                features = self.token_analyzer.prepare_features(token_metrics, [])
                
                if not features:
                    continue
                    
                feature_vector = []
                for category in ['basic_metrics', 'price_volume_metrics', 'holder_metrics', 
                               'sniper_metrics', 'dev_metrics', 'social_metrics', 
                               'security_metrics', 'wallet_behavior', 'risk_assessment']:
                    feature_vector.extend(list(features[category].values()))
                
                all_features.append(feature_vector)
                all_labels.append(1 if token in successful_tokens else 0)
                
            except Exception as e:
                logger.error(f"[2025-02-03 12:46:07] Error processing token {token['address']}: {e}")
                continue
        
        if not all_features:
            logger.error("[2025-02-03 12:46:07] No features to analyze")
            return
        
        # Konvertuojame ƒØ numpy arrays
        X = np.array(all_features)
        y = np.array(all_labels)
        
        # Skaiƒçiuojame koreliacijas
        feature_names = self._get_all_feature_names()
        correlations = []
        
        for i, feature in enumerate(feature_names):
            correlation = np.corrcoef(X[:, i], y)[0, 1]
            correlations.append((feature, correlation))
        
        # R≈´≈°iuojame pagal absoliuƒçiƒÖ koreliacijƒÖ
        correlations.sort(key=lambda x: abs(x[1]), reverse=True)
        
        logger.info("\n=== FEATURE CORRELATION ANALYSIS ===")
        logger.info("Top 20 Most Correlated Features with Gem Status:")
        
        for feature, correlation in correlations[:20]:
            direction = "POSITIVE" if correlation > 0 else "NEGATIVE"
            logger.info(f"‚Ä¢ {feature}: {abs(correlation):.3f} ({direction})")
        
        # Analizuojame feature kombinacijas
        logger.info("\nTop Feature Combinations:")
        
        top_features_idx = [feature_names.index(f[0]) for f in correlations[:5]]
        X_top = X[:, top_features_idx]
        
        success_features = X_top[y == 1]
        fail_features = X_top[y == 0]
        
        logger.info("\nTypical Successful Pattern:")
        for i, feature in enumerate(correlations[:5]):
            avg_success = np.mean(success_features[:, i])
            logger.info(f"‚Ä¢ {feature[0]}: {avg_success:.3f}")
        
        logger.info("\nTypical Failed Pattern:")
        for i, feature in enumerate(correlations[:5]):
            avg_fail = np.mean(fail_features[:, i])
            logger.info(f"‚Ä¢ {feature[0]}: {avg_fail:.3f}")
        
    except Exception as e:
        logger.error(f"[2025-02-03 12:46:07] Error analyzing feature correlations: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        
    

class DatabaseManager:
    def __init__(self):
        self.db = AsyncDatabase()
        self.is_setup = False

    async def setup_database(self):
        """Initialize database tables"""
        if not self.is_setup:
            await self.db.connect()  # Pirma prisijungiame
            self.is_setup = True
            logger.info(f"[{datetime.now(timezone.utc)}] Database setup completed")

    async def fetch_all(self, query: str, params=None):
        """Gauna visus rezultatus i≈° u≈æklausos"""
        try:
            if not self.db:
                await self.setup_database()
            # Perduodame params tik jei jie yra
            if params is not None:
                return await self.db.fetch_all(query, params)
            return await self.db.fetch_all(query)
        except Exception as e:
            logger.error(f"[2025-02-03 15:17:34] Error in DatabaseManager.fetch_all: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Params: {params}")
            raise

    async def fetch_one(self, query: str, params=None):
        """Gauna vienƒÖ rezultatƒÖ i≈° u≈æklausos"""
        try:
            if not self.db:
                await self.setup_database()
                
            # I≈°pakuojame tuple jei jis yra dvigubas
            if isinstance(params, tuple) and len(params) == 1 and isinstance(params[0], tuple):
                params = params[0]
            
            # I≈°pakuojame tuple jei jis yra viengubas su vienu elementu
            if isinstance(params, tuple) and len(params) == 1:
                params = params[0]
                
            return await self.db.fetch_one(query, params)
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error in DatabaseManager.fetch_one: {e}")
            logger.error(f"Query: {query}")
            logger.error(f"Params: {params}")
            raise
                 
               
        
        
    async def get_initial_state(self, address: str):
        """Gauna pradinƒô token'o b≈´senƒÖ"""
        query = """
        SELECT * FROM token_initial_states 
        WHERE address = ?
        """
        result = await self.db.fetch_one(query, address)
        if not result:
            return None
            
        # Konvertuojame ƒØ TokenMetrics objektƒÖ
        return TokenMetrics(
            address=result['address'],
            name=result['name'],
            symbol=result['symbol'],
            age=result['age'],
            market_cap=result['market_cap'],
            liquidity=result['liquidity'],
            volume_1h=result['volume_1h'],
            volume_24h=result['volume_24h'],
            price_change_1h=result['price_change_1h'],
            price_change_24h=result['price_change_24h'],
            mint_enabled=result['mint_enabled'],
            freeze_enabled=result['freeze_enabled'],
            lp_burnt_percentage=result['lp_burnt_percentage'],
            holders_count=result['holders_count'],
            top_holder_percentage=result['top_holder_percentage'],
            sniper_count=result['sniper_count'],
            sniper_percentage=result['sniper_percentage'],
            first_20_fresh=result['first_20_fresh'],
            ath_market_cap=result['ath_market_cap'],
            ath_multiplier=result['ath_multiplier'],
            owner_renounced=result['owner_renounced'],
            telegram_url=result['telegram_url'],
            twitter_url=result['twitter_url'],
            website_url=result['website_url'],
            dev_sol_balance=result['dev_sol_balance'],
            dev_token_percentage=result['dev_token_percentage'],
            sniper_wallets=json.loads(result['sniper_wallets']) if result['sniper_wallets'] else None 
        )
        
    async def save_initial_state(self, token_data: TokenMetrics):
        """I≈°saugo pradinƒØ token'o b≈´vƒØ"""
        base_data = (
            token_data.address, token_data.name, token_data.symbol, token_data.age,
            token_data.market_cap, token_data.liquidity, token_data.volume_1h, token_data.volume_24h,
            token_data.price_change_1h, token_data.price_change_24h,
            token_data.mint_enabled, token_data.freeze_enabled, token_data.lp_burnt_percentage,
            token_data.holders_count, token_data.top_holder_percentage,
            token_data.sniper_count, token_data.sniper_percentage, token_data.first_20_fresh,
            token_data.ath_market_cap, token_data.ath_multiplier,
            token_data.owner_renounced, token_data.telegram_url, token_data.twitter_url, token_data.website_url,
            token_data.dev_sol_balance, token_data.dev_token_percentage,
            datetime.now(timezone.utc),  # first_seen
            'new',  # status
            json.dumps(token_data.sniper_wallets) if token_data.sniper_wallets else None  # sniper_wallets
        )

        query = """
        INSERT OR REPLACE INTO token_initial_states (
            address, name, symbol, age,
            market_cap, liquidity, volume_1h, volume_24h,
            price_change_1h, price_change_24h,
            mint_enabled, freeze_enabled, lp_burnt_percentage,
            holders_count, top_holder_percentage,
            sniper_count, sniper_percentage, first_20_fresh,
            ath_market_cap, ath_multiplier,
            owner_renounced, telegram_url, twitter_url, website_url,
            dev_sol_balance, dev_token_percentage,
            first_seen, status, sniper_wallets
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        await self.db.execute(query, base_data)
        
    async def is_gem(self, address: str) -> bool:
        """Tikrina ar token'as jau pa≈æymƒótas kaip gem"""
        query = """
        SELECT status FROM token_initial_states 
        WHERE address = ?
        """
        result = await self.db.fetch_one(query, address)
        return result and result['status'] == 'gem'

    async def save_token_update(self, token_address: str, new_data: TokenMetrics):
        """I≈°saugo token'o atnaujinimƒÖ"""
        try:
            initial_data = await self.get_initial_state(token_address)
            if not initial_data:
                logger.warning(f"[{datetime.now(timezone.utc)}] No initial state found for token: {token_address}")
                return
                
            current_multiplier = new_data.market_cap / initial_data.market_cap
            
            # Konvertuojame ≈æodynƒÖ ƒØ JSON string
            metrics_json = json.dumps(new_data.to_dict())
            
            query = """
            INSERT INTO token_updates (
                address, all_metrics, timestamp, current_multiplier
            ) VALUES (?, ?, ?, ?)
            """
            await self.db.execute(query, (
                token_address, 
                metrics_json,  # Dabar perduodame JSON string vietoj ≈æodyno
                datetime.now(timezone.utc),
                current_multiplier
            ))
            
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error saving token update: {e}")
            raise

    async def save_sniper_wallets(self, wallets: List[Dict]):
        """I≈°saugo sniper wallet'us ƒØ DB"""
        try:
            for wallet in wallets:
                await self.db.execute("""
                    INSERT OR REPLACE INTO sniper_wallets (address, type)
                    VALUES (?, ?)
                """, (wallet['address'], wallet['type']))
                
            logger.info(f"[{datetime.now(timezone.utc)}] Saved {len(wallets)} sniper wallets to DB")
        except Exception as e:
            logger.error(f"Error saving sniper wallets: {e}")
        
    async def mark_as_gem(self, token_address: str):
        """Pa≈æymi token'ƒÖ kaip gem ir i≈°saugo ƒØ successful_tokens"""
        try:
            current_time = datetime.now(timezone.utc)
            
            # Patikrinam ar jau ne gem
            if await self.is_gem(token_address):
                logger.info(f"[2025-02-03 15:01:59] Token {token_address} is already marked as gem")
                return False
                
            # Patikriname ar jau yra successful_tokens lentelƒóje
            check_query = "SELECT address FROM successful_tokens WHERE address = ?"
            existing = await self.db.fetch_one(check_query, token_address)
            
            if existing:
                logger.info(f"[2025-02-03 15:01:59] Token {token_address} already in successful_tokens")
                return True
                
            # 1. Atnaujina statusƒÖ
            await self.db.execute(
                "UPDATE token_initial_states SET status = 'gem' WHERE address = ?",
                token_address
            )
            logger.info(f"[2025-02-03 15:01:59] Updated status to gem for {token_address}")
            
            # 2. Gauna pradinƒô b≈´senƒÖ ir laikƒÖ iki 10x
            initial_data = await self.get_initial_state(token_address)
            if not initial_data:
                logger.error(f"[2025-02-03 15:01:59] No initial state found for {token_address}")
                return False
                
            time_to_10x = await self._calculate_time_to_10x(token_address)
            logger.info(f"[2025-02-03 15:01:59] Calculated time to 10x for {token_address}: {time_to_10x} seconds")
            
            try:
                # 3. I≈°saugo ƒØ successful_tokens su REPLACE INTO vietoj INSERT
                query = """
                REPLACE INTO successful_tokens (
                    address, initial_parameters, time_to_10x, discovery_timestamp
                ) VALUES (?, ?, ?, ?)
                """
                
                # Konvertuojame ƒØ JSON string
                initial_parameters_json = json.dumps(initial_data.to_dict())
                
                await self.db.execute(query, (
                    token_address,
                    initial_parameters_json,
                    time_to_10x,
                    current_time
                ))
                
                logger.info(f"[2025-02-03 15:01:59] Successfully marked {token_address} as gem")
                return True
                
            except Exception as insert_error:
                logger.error(f"[2025-02-03 15:01:59] Error inserting into successful_tokens: {insert_error}")
                return False
                
        except Exception as e:
            logger.error(f"[2025-02-03 15:01:59] Error marking as gem: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return False
        
    async def get_all_gems(self):
        """Gauna visus sƒókmingus tokenus mokymui"""
        try:
            query = """
            SELECT * FROM successful_tokens
            ORDER BY discovery_timestamp DESC
            """
            results = await self.db.fetch_all(query)
            logger.info(f"[{datetime.now(timezone.utc)}] Found {len(results) if results else 0} gems in database")
            return results
            
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error getting gems: {e}")
            return []

    async def get_active_tokens(self):
        """Gauna visus aktyvius stebimus tokenus"""
        query = """
        SELECT * FROM token_initial_states 
        WHERE status = 'new'
        """
        return await self.db.fetch_all(query)

    async def check_database(self):
        """Patikrina duomen≈≥ bazƒós b≈´senƒÖ"""
        try:
            current_time = datetime.now(timezone.utc)
            
            # Tikriname token_initial_states
            initial_states = await self.db.fetch_all("""
                SELECT COUNT(*) as count, status, COUNT(DISTINCT address) as unique_addresses 
                FROM token_initial_states 
                GROUP BY status
            """)
            
            # Tikriname nesutapimus tarp lenteli≈≥
            inconsistencies = await self.db.fetch_all("""
                SELECT t.address, t.status, 
                    CASE 
                        WHEN t.status = 'gem' AND s.address IS NULL THEN 'Missing from successful_tokens'
                        WHEN t.status = 'failed' AND f.address IS NULL THEN 'Missing from failed_tokens'
                        ELSE NULL 
                    END as issue
                FROM token_initial_states t
                LEFT JOIN successful_tokens s ON t.address = s.address
                LEFT JOIN failed_tokens f ON t.address = f.address
                WHERE (t.status = 'gem' AND s.address IS NULL)
                   OR (t.status = 'failed' AND f.address IS NULL)
            """)
            
            if inconsistencies:
                logger.warning(f"[{current_time}] Found data inconsistencies:")
                for inc in inconsistencies:
                    logger.warning(f"[{current_time}] Address {inc['address']} ({inc['status']}): {inc['issue']}")
                    
                    # Automati≈°kai pataisome
                    if inc['status'] == 'gem':
                        initial_data = await self.get_initial_state(inc['address'])
                        if initial_data:
                            await self.db.execute("""
                                INSERT INTO successful_tokens (
                                    address, initial_parameters, time_to_10x, discovery_timestamp
                                ) VALUES (?, ?, ?, ?)
                            """, (
                                inc['address'],
                                json.dumps(initial_data.to_dict()),
                                0,
                                current_time
                            ))
                            logger.info(f"[{current_time}] Fixed missing gem: {inc['address']}")

            # Rodyti statistikƒÖ
            for state in initial_states:
                logger.info(f"[{current_time}] Initial states - Status: {state['status']}, "
                           f"Count: {state['count']}, Unique addresses: {state['unique_addresses']}")

            # Tikriname successful_tokens
            successful = await self.db.fetch_all("SELECT COUNT(*) as count FROM successful_tokens")
            failed = await self.db.fetch_all("SELECT COUNT(*) as count FROM failed_tokens")  # Pridƒóta failed tokens statistika
            
            logger.info(f"[{current_time}] Successful tokens count: {successful[0]['count']}")
            logger.info(f"[{current_time}] Failed tokens count: {failed[0]['count']}")  # Naujas log'as

            # Tikriname token_updates
            updates = await self.db.fetch_all("""
                SELECT COUNT(*) as count, COUNT(DISTINCT address) as unique_addresses 
                FROM token_updates
            """)
            logger.info(f"[{current_time}] Updates count: {updates[0]['count']}, "
                       f"Unique addresses: {updates[0]['unique_addresses']}")

            # Patikriname duomen≈≥ bazƒós dydƒØ ir veikimƒÖ
            logger.info(f"[{current_time}] Database health check completed successfully")

        except Exception as e:
            logger.error(f"[{current_time}] Error checking database: {e}")
    
    async def _calculate_time_to_10x(self, token_address: str) -> int:
        """Apskaiƒçiuoja laikƒÖ (sekundƒómis) per kurƒØ token'as pasiekƒó 10x"""
        try:
            # Gauname first_seen tiesiai i≈° token_initial_states
            first_seen_query = """
            SELECT first_seen 
            FROM token_initial_states 
            WHERE address = ?
            """
            initial_result = await self.db.fetch_one(first_seen_query, token_address)
            if not initial_result:
                logger.warning(f"[2025-02-01 21:45:44] No initial state found for {token_address}")
                return 0

            initial_time = datetime.fromisoformat(str(initial_result['first_seen']))

            # Gauname atnaujinimus
            updates_query = """
            SELECT timestamp, current_multiplier 
            FROM token_updates 
            WHERE address = ? 
            ORDER BY timestamp ASC
            """
            updates = await self.db.fetch_all(updates_query, token_address)
            
            if not updates:
                return 0

            # Ie≈°kome pirmo 10x
            for update in updates:
                if update['current_multiplier'] >= 10:
                    update_time = datetime.fromisoformat(str(update['timestamp']))
                    return int((update_time - initial_time).total_seconds())
            
            return 0
            
        except Exception as e:
            logger.error(f"[2025-02-01 21:45:44] Error calculating time to 10x: {e}")
            return 0

    async def show_database_contents(self):
        """Parodo VISUS duomenis i≈° duomen≈≥ bazƒós"""
        try:
            # 1. Parodome token_initial_states su VISAIS stulpeliais
            logger.info("\n=== TOKEN INITIAL STATES (FULL DATA) ===")
            initial_states = await self.db.fetch_all("""
                SELECT * FROM token_initial_states 
                ORDER BY first_seen DESC
            """)
            for token in initial_states:
                logger.info(f"\nToken Details:")
                for column, value in dict(token).items():
                    logger.info(f"{column}: {value}")
                logger.info("------------------------")

            # 2. Parodome successful_tokens su VISAIS duomenimis
            logger.info("\n=== SUCCESSFUL TOKENS (GEMS) - FULL DATA ===")
            successful = await self.db.fetch_all("""
                SELECT * FROM successful_tokens
                ORDER BY discovery_timestamp DESC
            """)
            for gem in successful:
                logger.info(f"\nGem Details:")
                for column, value in dict(gem).items():
                    if column == 'initial_parameters':
                        # Jei JSON, i≈°skaidome ƒØ atskiras eilutes
                        params = json.loads(value)
                        logger.info("Initial Parameters:")
                        for param_key, param_value in params.items():
                            logger.info(f"  {param_key}: {param_value}")
                    else:
                        logger.info(f"{column}: {value}")
                logger.info("------------------------")

            # 3. Parodome VISUS token_updates 
            logger.info("\n=== ALL TOKEN UPDATES - FULL DATA ===")
            updates = await self.db.fetch_all("""
                SELECT * FROM token_updates 
                ORDER BY timestamp DESC
            """)
            for update in updates:
                logger.info(f"\nUpdate Details:")
                for column, value in dict(update).items():
                    if column == 'all_metrics':
                        # Jei JSON, i≈°skaidome ƒØ atskiras eilutes
                        metrics = json.loads(value)
                        logger.info("All Metrics:")
                        for metric_key, metric_value in metrics.items():
                            logger.info(f"  {metric_key}: {metric_value}")
                    else:
                        logger.info(f"{column}: {value}")
                logger.info("------------------------")

            # 4. ƒåIAAAAAA pridedame sniper_wallets rodymƒÖ! üëá
            logger.info("\n=== SNIPER WALLETS - FULL DATA ===")
            sniper_wallets = await self.db.fetch_all("""
                SELECT * FROM sniper_wallets 
                ORDER BY last_active DESC
            """)
            for wallet in sniper_wallets:
                logger.info(f"\nSniper Wallet Details:")
                for column, value in dict(wallet).items():
                    logger.info(f"{column}: {value}")
                logger.info("------------------------")

        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error showing database contents: {e}")
            logger.error(f"Exception details: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

                
    async def get_failed_tokens(self):
        """Gauna visus nepavykusius tokenus"""
        try:
            query = """
            SELECT * FROM failed_tokens
            ORDER BY discovery_timestamp DESC
            """
            results = await self.db.fetch_all(query)
            logger.info(f"[{datetime.now(timezone.utc)}] Found {len(results) if results else 0} failed tokens")
            return results
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error getting failed tokens: {e}")
            return []
    async def get_latest_token_update(self, token_address: str):
        """Gauna paskutinƒØ token'o atnaujinimƒÖ i≈° DB"""
        try:
            query = """
            SELECT all_metrics 
            FROM token_updates 
            WHERE address = ? 
            ORDER BY timestamp DESC 
            LIMIT 1
            """
            result = await self.db.fetch_one(query, token_address)
            if result:
                return json.loads(result['all_metrics'])
            return None
        except Exception as e:
            logger.error(f"[2025-02-03 18:07:45] Error getting latest token update: {e}")
            return None
            
    async def mark_as_failed(self, token_address: str, reason: str):
        """Pa≈æymi token'ƒÖ kaip nepavykusƒØ"""
        try:
            # 1. Gauname pradinƒô b≈´senƒÖ
            initial_data = await self.get_initial_state(token_address)
            if not initial_data:
                return
                
            # 2. Atnaujiname statusƒÖ
            await self.db.execute(
                "UPDATE token_initial_states SET status = 'failed' WHERE address = ?",
                token_address
            )
            
            # 3. ƒÆra≈°ome ƒØ failed_tokens
            query = """
            INSERT INTO failed_tokens (
                address, initial_parameters, failure_reason, discovery_timestamp
            ) VALUES (?, ?, ?, ?)
            """
            await self.db.execute(query, (
                token_address,
                json.dumps(initial_data.to_dict()),
                reason,
                datetime.now(timezone.utc)
            ))
            
            logger.info(f"[{datetime.now(timezone.utc)}] Token marked as failed: {token_address} - {reason}")
            
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error marking token as failed: {e}")

    

class TokenAnalytics:
    def __init__(self, db_manager):
        self.db = db_manager
        
    async def analyze_success_patterns(self):
        """Analizuoja sƒókming≈≥ token≈≥ ≈°ablonus"""
        gems = await self.db.get_all_gems()
        
        patterns = {
            'market_patterns': await self._analyze_market_metrics(gems),
            'security_patterns': await self._analyze_security_metrics(gems),
            'community_patterns': await self._analyze_community_metrics(gems),
            'dev_patterns': await self._analyze_dev_metrics(gems)
        }
        
        return patterns
        
    async def compare_with_successful(self, token_data: TokenMetrics):
        """Lygina naujƒÖ token'ƒÖ su sƒókmingais"""
        patterns = await self.analyze_success_patterns()
        similarity_score = self._calculate_similarity(token_data, patterns)
        return similarity_score

    async def _analyze_market_metrics(self, gems):
        """Analizuoja rinkos metrikas"""
        market_metrics = []
        for gem in gems:
            market_metrics.append({
                'liquidity_ratio': gem.liquidity / gem.market_cap if gem.market_cap else 0,
                'volume_ratio': gem.volume_24h / gem.market_cap if gem.market_cap else 0,
                'price_change': gem.price_change_24h
            })
        return market_metrics

    async def _analyze_security_metrics(self, gems):
        """Analizuoja saugumo metrikas"""
        return {
            'mint_enabled_ratio': sum(1 for g in gems if g.mint_enabled) / len(gems),
            'freeze_enabled_ratio': sum(1 for g in gems if g.freeze_enabled) / len(gems),
            'avg_lp_burnt': sum(g.lp_burnt_percentage for g in gems) / len(gems)
        }



class TokenAnalyzer:
    def __init__(self, db_manager, ml_analyzer):
        self.db = db_manager
        self.ml = ml_analyzer

    def prepare_features(self, token: TokenMetrics, updates: List[TokenUpdate]) -> Dict:
        """Paruo≈°ia visus features ML modeliui"""
        try:
            if token is None:
                logger.error(f"[2025-02-01 21:17:42] Token is None")
                return None

            try:
                return {
                    # 1. Pagrindinƒó informacija ir jos analizƒó
                    'basic_metrics': self._analyze_basic_metrics(token),

                    # Pridedame naujƒÖ kategorijƒÖ
                    'scan_metrics': {
                        'scan_count': token.total_scans,
                        'scan_intensity': token.total_scans / max(self._convert_age_to_hours(token.age), 1),  # Scans per hour
                        'scan_momentum': 1.0 if token.total_scans > 100 else token.total_scans / 100.0  # Normalizuotas scan_count
                    },
                    
                    # 2. Kain≈≥ ir volume analizƒó
                    'price_volume_metrics': self._analyze_price_volume(token, updates),
                    
                    # 3. Holders analizƒó
                    'holder_metrics': self._analyze_holders(token, updates),
                    
                    # 4. Snipers analizƒó
                    'sniper_metrics': self._analyze_snipers(token),
                    
                    # 5. Dev/Owner analizƒó
                    'dev_metrics': self._analyze_dev_metrics(token),
                    
                    # 6. Socialini≈≥ metrik≈≥ analizƒó
                    'social_metrics': self._analyze_social_metrics(token),
                    
                    # 7. Saugumo metrik≈≥ analizƒó
                    'security_metrics': self._analyze_security(token),
                    
                    # 8. Laiko serijos analizƒó
                    'time_series_metrics': self._analyze_time_series(updates),
                    
                    # 9. Wallet'≈≥ elgsenos analizƒó
                    'wallet_behavior': self._analyze_wallets(token.sniper_wallets if token.sniper_wallets else []),
                    
                    # 10. Rizikos vertinimas
                    'risk_assessment': self._calculate_risk_metrics(token)
                }
                
            except Exception as e:
                logger.error(f"[2025-02-01 21:17:42] Error analyzing features for {token.address}: {str(e)}")
                return None

        except Exception as e:
            logger.error(f"[2025-02-01 21:17:42] Error preparing features: {str(e)}")
            return None

    def _analyze_basic_metrics(self, token: TokenMetrics) -> Dict:
        """Analizuoja pagrindinius metrikus"""
        return {
            'age_hours': self._convert_age_to_hours(token.age),
            'market_cap_normalized': token.market_cap / token.ath_market_cap if token.ath_market_cap else 0,
            'liquidity_ratio': token.liquidity / token.market_cap if token.market_cap else 0
        }

    def _analyze_price_volume(self, token: TokenMetrics, updates: List[TokenUpdate]) -> Dict:
        """Analizuoja kainos ir volume metrikas"""
        return {
            'price_momentum': self._calculate_momentum(updates),
            'volume_trend': self._analyze_volume_trend(updates),
            'price_volatility': self._calculate_volatility(updates),
            'volume_consistency': self._analyze_volume_consistency(updates),
            'mcap_to_volume_ratio': token.market_cap / max(token.volume_24h, 1)
        }

    def _analyze_holders(self, token: TokenMetrics, updates: List[TokenUpdate]) -> Dict:
        """Analizuoja holder'i≈≥ metrikas"""
        return {
            'holder_growth_rate': self._calculate_holder_growth(updates),
            'holder_distribution': self._analyze_holder_distribution(token),
            'value_per_holder': token.market_cap / max(token.holders_count, 1),
            'holder_retention': self._calculate_holder_retention(updates),
            'top_holder_risk': self._analyze_top_holder_risk(token)
        }

    def _analyze_snipers(self, token: TokenMetrics) -> Dict:
        """Analizuoja sniperi≈≥ metrikas"""
        try:
            if token is None or token.sniper_wallets is None:
                token.sniper_wallets = []
                logger.warning(f"[2025-02-01 21:32:03] sniper_wallets is None for {token.address}")

            sniper_types = self._categorize_snipers(token.sniper_wallets)
            wallet_count = len(token.sniper_wallets) if token.sniper_wallets else 1
            
            return {
                'sniper_impact': token.sniper_percentage / 100,
                'whale_dominance': sniper_types['whale_count'] / max(wallet_count, 1),
                'fish_ratio': sniper_types['fish_count'] / max(wallet_count, 1),
                'sniper_diversity': self._calculate_sniper_diversity(token.sniper_wallets),
                'sniper_behavior_pattern': self._analyze_sniper_patterns(token.sniper_wallets)
            }

        except Exception as e:
            logger.error(f"[2025-02-01 21:32:03] Error in _analyze_snipers for {token.address}: {str(e)}")
            return {
                'sniper_impact': 0.0,
                'whale_dominance': 0.0,
                'fish_ratio': 0.0,
                'sniper_diversity': 0.0,
                'sniper_behavior_pattern': 0.0
            }

    def _analyze_dev_metrics(self, token: TokenMetrics) -> Dict:
        """Analizuoja dev/owner metrikas"""
        return {
            'dev_commitment': self._calculate_dev_commitment(token),
            'owner_risk': self._assess_owner_risk(token),
            'dev_sol_strength': self._normalize_sol_balance(token.dev_sol_balance),
            'dev_token_risk': token.dev_token_percentage / 100,
            'ownership_score': self._calculate_ownership_score(token)
        }

    def _analyze_security(self, token: TokenMetrics) -> Dict:
        """Analizuoja saugumo metrikas"""
        return {
            'contract_security': self._assess_contract_security(token),
            'lp_security': token.lp_burnt_percentage / 100,
            'mint_risk': 1 if token.mint_enabled else 0,
            'freeze_risk': 1 if token.freeze_enabled else 0,
            'overall_security_score': self._calculate_security_score(token)
        }

    def _analyze_time_series(self, updates: List[TokenUpdate]) -> Dict:
        """Analizuoja laiko serijos duomenis"""
        return {
            'price_trend': self._calculate_price_trend(updates),
            'volume_pattern': self._identify_volume_pattern(updates),
            'holder_trend': self._analyze_holder_trend(updates),
            'growth_stability': self._calculate_growth_stability(updates),
            'momentum_indicators': self._calculate_momentum_indicators(updates)
        }

    def _analyze_wallets(self, wallets: List[Dict]) -> Dict:
        """Analizuoja wallet'≈≥ elgsenƒÖ"""
        return {
            'wallet_diversity': self._calculate_wallet_diversity(wallets),
            'interaction_patterns': self._analyze_wallet_interactions(wallets),
            'whale_behavior': self._analyze_whale_behavior(wallets),
            'wallet_age_distribution': self._analyze_wallet_ages(wallets),
            'wallet_risk_score': self._calculate_wallet_risk(wallets)
        }

    def _calculate_risk_metrics(self, token: TokenMetrics) -> Dict:
        """Apskaiƒçiuoja rizikos metrikus"""
        return {
            'overall_risk': self._calculate_overall_risk(token),
            'pump_dump_risk': self._assess_pump_dump_risk(token),
            'security_risk': self._assess_security_risk(token),
            'holder_risk': self._assess_holder_risk(token),
            'dev_risk': self._assess_dev_risk(token)
        }
    def _convert_age_to_hours(self, age: str) -> float:
        """Konvertuoja am≈æi≈≥ ƒØ valandas"""
        try:
            number = int(''.join(filter(str.isdigit, age)))
            if 'm' in age:
                return number / 60  # minutƒós ƒØ valandas
            elif 'h' in age:
                return number  # jau valandos
            elif 'd' in age:
                return number * 24  # dienos ƒØ valandas
            return 0
        except:
            return 0

    def _calculate_momentum(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja kainos momentum"""
        if not updates:
            return 0.0
        try:
            # Imame paskutines 5 atnaujinimus
            recent = updates[-5:]
            price_changes = [u.price_change_1h for u in recent]
            return sum(price_changes) / len(price_changes)
        except:
            return 0.0

    def _analyze_volume_trend(self, updates: List[TokenUpdate]) -> float:
        """Analizuoja volume trendƒÖ"""
        if not updates:
            return 0.0
        try:
            volumes = [u.volume_1h for u in updates]
            if len(volumes) < 2:
                return 0.0
            # Skaiƒçiuojame volume pokytƒØ
            return (volumes[-1] - volumes[0]) / volumes[0] if volumes[0] > 0 else 0.0
        except:
            return 0.0

    def _calculate_volatility(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja kainos volatility"""
        if not updates:
            return 0.0
        try:
            price_changes = [abs(u.price_change_1h) for u in updates]
            return np.std(price_changes) if price_changes else 0.0
        except:
            return 0.0

    def _analyze_volume_consistency(self, updates: List[TokenUpdate]) -> float:
        """Analizuoja volume pastovumƒÖ"""
        if not updates:
            return 0.0
        try:
            volumes = [u.volume_1h for u in updates]
            avg_volume = np.mean(volumes)
            std_volume = np.std(volumes)
            return std_volume / avg_volume if avg_volume > 0 else 0.0
        except:
            return 0.0

    def _calculate_holder_growth(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja holder'i≈≥ augimƒÖ"""
        if not updates:
            return 0.0
        try:
            holders = [u.holders_count for u in updates]
            if len(holders) < 2:
                return 0.0
            return (holders[-1] - holders[0]) / max(holders[0], 1)
        except:
            return 0.0

    def _analyze_holder_distribution(self, token: TokenMetrics) -> float:
        """Analizuoja holder'i≈≥ pasiskirstymƒÖ"""
        try:
            return 1.0 - (token.top_holder_percentage / 100.0)  # Kuo ma≈æesnƒó koncentracija, tuo geriau
        except:
            return 0.0

    def _calculate_holder_retention(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja holder'i≈≥ i≈°laikymƒÖ"""
        if not updates:
            return 0.0
        try:
            holder_counts = [u.holders_count for u in updates]
            drops = sum(1 for i in range(1, len(holder_counts)) if holder_counts[i] < holder_counts[i-1])
            return 1.0 - (drops / max(len(updates)-1, 1))
        except:
            return 0.0

    def _categorize_snipers(self, wallets: List[Dict]) -> Dict:
        """Kategorizuoja sniper'ius"""
        result = {'whale_count': 0, 'fish_count': 0}
        if not wallets:
            return result
        try:
            for wallet in wallets:
                if wallet['type'] in ['üê≥']:  # Whale emoji
                    result['whale_count'] += 1
                elif wallet['type'] in ['üêü', 'üç§']:  # Fish emoji
                    result['fish_count'] += 1
            return result
        except:
            return result

    def _calculate_sniper_diversity(self, wallets: List[Dict]) -> float:
        """Skaiƒçiuoja sniper'i≈≥ ƒØvairovƒô"""
        if not wallets:
            return 0.0
        try:
            types = [w['type'] for w in wallets]
            unique_types = len(set(types))
            return unique_types / len(wallets)
        except:
            return 0.0

    def _analyze_sniper_patterns(self, wallets: List[Dict]) -> float:
        """Analizuoja sniper'i≈≥ elgsenos ≈°ablonus"""
        if not wallets:
            return 0.0
        try:
            # Skaiƒçiuojame whales/fish santykƒØ
            categories = self._categorize_snipers(wallets)
            total = categories['whale_count'] + categories['fish_count']
            if total == 0:
                return 0.0
            return categories['fish_count'] / total  # Didesnis fish ratio = geriau
        except:
            return 0.0

    def _calculate_dev_commitment(self, token: TokenMetrics) -> float:
        """Vertina dev commitment"""
        try:
            # Vertiname pagal SOL balansƒÖ ir token %
            sol_score = min(token.dev_sol_balance / 100, 1.0)  # Normalizuojame iki 100 SOL
            token_score = token.dev_token_percentage / 100
            return (sol_score + token_score) / 2
        except:
            return 0.0

    def _assess_owner_risk(self, token: TokenMetrics) -> float:
        """Vertina owner rizikƒÖ"""
        try:
            if token.owner_renounced:
                return 0.0  # Ma≈æiausia rizika
            return token.dev_token_percentage / 100  # Rizika proporcinga dev token %
        except:
            return 1.0

    def _normalize_sol_balance(self, balance: float) -> float:
        """Normalizuoja SOL balansƒÖ"""
        try:
            return min(balance / 100, 1.0)  # Normalizuojame iki 100 SOL
        except:
            return 0.0

    def _calculate_ownership_score(self, token: TokenMetrics) -> float:
        """Skaiƒçiuoja ownership score"""
        try:
            # Vertiname pagal kelis faktorius
            renounced_score = 1.0 if token.owner_renounced else 0.0
            dev_token_score = 1.0 - (token.dev_token_percentage / 100)
            return (renounced_score + dev_token_score) / 2
        except:
            return 0.0

    def _assess_contract_security(self, token: TokenMetrics) -> float:
        """Vertina kontrakto saugumƒÖ"""
        try:
            # Skaiƒçiuojame bendrƒÖ saugumo score
            mint_score = 0.0 if token.mint_enabled else 1.0
            freeze_score = 0.0 if token.freeze_enabled else 1.0
            lp_score = token.lp_burnt_percentage / 100
            return (mint_score + freeze_score + lp_score) / 3
        except:
            return 0.0

    def _calculate_security_score(self, token: TokenMetrics) -> float:
        """Skaiƒçiuoja bendrƒÖ saugumo score"""
        try:
            contract_security = self._assess_contract_security(token)
            ownership_security = self._calculate_ownership_score(token)
            lp_security = token.lp_burnt_percentage / 100
            return (contract_security + ownership_security + lp_security) / 3
        except:
            return 0.0

    def _analyze_social_metrics(self, token: TokenMetrics) -> Dict:
        """Analizuoja socialinius metrikus"""
        try:
            # Skaiƒçiuojame social presence score
            has_twitter = 1.0 if token.twitter_url else 0.0
            has_website = 1.0 if token.website_url else 0.0
            has_telegram = 1.0 if token.telegram_url else 0.0
            
            social_score = (has_twitter + has_website + has_telegram) / 3
            
            return {
                'social_presence': social_score,
                'has_twitter': has_twitter,
                'has_website': has_website,
                'has_telegram': has_telegram,
                'social_risk': 1.0 - social_score
            }
        except:
            return {
                'social_presence': 0.0,
                'has_twitter': 0.0,
                'has_website': 0.0,
                'has_telegram': 0.0,
                'social_risk': 1.0
            }

    def _analyze_wallet_interactions(self, wallets: List[Dict]) -> Dict:
        """Analizuoja wallet'≈≥ tarpusavio sƒÖveikas"""
        try:
            if not wallets:
                return {'interaction_score': 0.0, 'interaction_pattern': 'none'}
                
            # Skaiƒçiuojame wallet tip≈≥ pasiskirstymƒÖ
            type_counts = {'üê≥': 0, 'üêü': 0, 'üç§': 0, 'üå±': 0}
            for wallet in wallets:
                if wallet['type'] in type_counts:
                    type_counts[wallet['type']] += 1
                    
            total = sum(type_counts.values())
            if total == 0:
                return {'interaction_score': 0.0, 'interaction_pattern': 'none'}
                
            # Vertiname sƒÖveikos ≈°ablonƒÖ
            whale_ratio = type_counts['üê≥'] / total if total > 0 else 0
            fish_ratio = (type_counts['üêü'] + type_counts['üç§']) / total if total > 0 else 0
            
            # Nustatome sƒÖveikos tipƒÖ
            if whale_ratio > 0.5:
                pattern = 'whale_dominated'
                score = 0.3  # Rizikinga
            elif fish_ratio > 0.7:
                pattern = 'distributed'
                score = 0.8  # Gerai
            else:
                pattern = 'mixed'
                score = 0.5  # Neutralu
                
            return {
                'interaction_score': score,
                'interaction_pattern': pattern,
                'whale_ratio': whale_ratio,
                'fish_ratio': fish_ratio
            }
        except:
            return {
                'interaction_score': 0.0,
                'interaction_pattern': 'error',
                'whale_ratio': 0.0,
                'fish_ratio': 0.0
            }

    def _calculate_price_trend(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja kainos trendƒÖ"""
        if not updates:
            return 0.0
        try:
            price_changes = [u.price_change_1h for u in updates]
            return sum(price_changes) / len(price_changes)
        except:
            return 0.0

    def _identify_volume_pattern(self, updates: List[TokenUpdate]) -> float:
        """Identifikuoja volume ≈°ablonƒÖ"""
        if not updates:
            return 0.0
        try:
            volumes = [u.volume_1h for u in updates]
            if len(volumes) < 3:
                return 0.0
            # Analizuojame volume trendƒÖ
            increases = sum(1 for i in range(1, len(volumes)) if volumes[i] > volumes[i-1])
            return increases / (len(volumes) - 1)
        except:
            return 0.0

    def _analyze_holder_trend(self, updates: List[TokenUpdate]) -> float:
        """Analizuoja holder'i≈≥ trendƒÖ"""
        if not updates:
            return 0.0
        try:
            holders = [u.holders_count for u in updates]
            if len(holders) < 2:
                return 0.0
            return (holders[-1] - holders[0]) / max(holders[0], 1)
        except:
            return 0.0

    def _calculate_growth_stability(self, updates: List[TokenUpdate]) -> float:
        """Skaiƒçiuoja augimo stabilumƒÖ"""
        if not updates:
            return 0.0
        try:
            changes = [abs(u.price_change_1h) for u in updates]
            return 1.0 - (np.std(changes) / max(np.mean(changes), 0.0001))
        except:
            return 0.0

    def _calculate_wallet_diversity(self, wallets: List[Dict]) -> float:
        """Skaiƒçiuoja wallet'≈≥ ƒØvairovƒô"""
        try:
            if not wallets:
                return 0.0
                
            # Skaiƒçiuojame kiekvieno tipo wallet'≈≥ skaiƒçi≈≥
            type_counts = {'üê≥': 0, 'üêü': 0, 'üç§': 0, 'üå±': 0}
            for wallet in wallets:
                if wallet['type'] in type_counts:
                    type_counts[wallet['type']] += 1
                    
            # Skaiƒçiuojame diversity score
            total_wallets = sum(type_counts.values())
            if total_wallets == 0:
                return 0.0
                
            # Shannon Diversity Index
            proportions = [count/total_wallets for count in type_counts.values() if count > 0]
            diversity = -sum(p * math.log(p) for p in proportions)
            
            # Normalizuojame ƒØ [0,1] intervalƒÖ
            max_diversity = math.log(len(type_counts))  # Maximum possible diversity
            normalized_diversity = diversity / max_diversity if max_diversity > 0 else 0.0
            
            return normalized_diversity
            
        except Exception as e:
            logger.error(f"[2025-02-01 21:26:24] Error calculating wallet diversity: {str(e)}")
            return 0.0

    def _calculate_momentum_indicators(self, updates: List[TokenUpdate]) -> Dict:
        """Skaiƒçiuoja momentum indikatorius"""
        if not updates:
            return {'momentum': 0.0, 'acceleration': 0.0}
        try:
            price_changes = [u.price_change_1h for u in updates]
            momentum = sum(price_changes) / len(price_changes)
            
            # Skaiƒçiuojame acceleration (pokytis momentum'e)
            if len(price_changes) > 1:
                acc = (price_changes[-1] - price_changes[0]) / len(price_changes)
            else:
                acc = 0.0
                
            return {'momentum': momentum, 'acceleration': acc}
        except:
            return {'momentum': 0.0, 'acceleration': 0.0}

    def _analyze_whale_behavior(self, wallets: List[Dict]) -> Dict:
        """Analizuoja bangini≈≥ elgsenƒÖ"""
        try:
            whales = [w for w in wallets if w['type'] == 'üê≥']
            if not whales:
                return {'whale_activity': 0.0, 'risk_level': 'low'}
                
            whale_ratio = len(whales) / len(wallets)
            
            if whale_ratio > 0.5:
                risk = 'high'
            elif whale_ratio > 0.3:
                risk = 'medium'
            else:
                risk = 'low'
                
            return {
                'whale_activity': whale_ratio,
                'risk_level': risk
            }
        except:
            return {'whale_activity': 0.0, 'risk_level': 'error'}

    def _analyze_wallet_ages(self, wallets: List[Dict]) -> Dict:
        """Analizuoja wallet'≈≥ am≈æi≈≥"""
        try:
            if not wallets:
                return {'avg_age': 0.0, 'new_wallet_ratio': 0.0}
                
            # ≈†iuo atveju neturime wallet age info, tai grƒÖ≈æiname default
            return {
                'avg_age': 0.0,
                'new_wallet_ratio': 0.0
            }
        except:
            return {'avg_age': 0.0, 'new_wallet_ratio': 0.0}

    def _calculate_wallet_risk(self, wallets: List[Dict]) -> float:
        """Skaiƒçiuoja bendrƒÖ wallet rizikƒÖ"""
        try:
            if not wallets:
                return 1.0  # Auk≈°ƒçiausia rizika jei nƒóra wallet'≈≥
                
            # Skaiƒçiuojame risk score
            whale_behavior = self._analyze_whale_behavior(wallets)
            interactions = self._analyze_wallet_interactions(wallets)
            
            risk_score = (
                float(whale_behavior['whale_activity']) * 0.4 +
                (1.0 - float(interactions['interaction_score'])) * 0.6
            )
            
            return risk_score
        except:
            return 1.0

    def _analyze_top_holder_risk(self, token: TokenMetrics) -> float:
        """Analizuoja top holder'i≈≥ rizikƒÖ"""
        try:
            # Kuo didesnƒó top holder koncentracija, tuo didesnƒó rizika
            return token.top_holder_percentage / 100
        except:
            return 1.0

    def _assess_dev_risk(self, token: TokenMetrics) -> float:
        """Vertina dev rizikƒÖ"""
        try:
            # Vertiname pagal kelis faktorius
            token_risk = token.dev_token_percentage / 100
            renounced_bonus = 0.0 if token.owner_renounced else 0.3
            sol_balance_factor = 1.0 - self._normalize_sol_balance(token.dev_sol_balance)
            
            return (token_risk + renounced_bonus + sol_balance_factor) / 3
        except:
            return 1.0

    def _calculate_overall_risk(self, token: TokenMetrics) -> float:
        """Skaiƒçiuoja bendrƒÖ rizikos ƒØvertinimƒÖ"""
        try:
            # Renkame visus rizikos faktorius
            security_risk = self._assess_security_risk(token)
            holder_risk = self._assess_holder_risk(token)
            dev_risk = self._assess_dev_risk(token)
            pump_dump_risk = self._assess_pump_dump_risk(token)
            
            # Skaiƒçiuojame svertinƒØ vidurkƒØ
            weights = {
                'security': 0.3,
                'holder': 0.2,
                'dev': 0.3,
                'pump_dump': 0.2
            }
            
            overall = (
                security_risk * weights['security'] +
                holder_risk * weights['holder'] +
                dev_risk * weights['dev'] +
                pump_dump_risk * weights['pump_dump']
            )
            
            return overall
        except:
            return 1.0

    def _assess_pump_dump_risk(self, token: TokenMetrics) -> float:
        """Vertina pump&dump rizikƒÖ"""
        try:
            # Vertiname pagal kelis faktorius
            volume_volatility = token.volume_1h / max(token.volume_24h / 24, 0.0001)
            price_volatility = abs(token.price_change_1h)
            holder_concentration = token.top_holder_percentage / 100
            
            # Skaiƒçiuojame risk score
            risk_factors = [
                volume_volatility > 3.0,  # Staigus volume padidƒójimas
                price_volatility > 50.0,  # Didelis kainos svyravimas
                holder_concentration > 0.5  # Didelƒó holder koncentracija
            ]
            
            return sum(risk_factors) / len(risk_factors)
        except:
            return 1.0

    def _assess_security_risk(self, token: TokenMetrics) -> float:
        """Vertina saugumo rizikƒÖ"""
        try:
            # Renkame saugumo faktorius
            contract_security = self._assess_contract_security(token)
            ownership_security = self._calculate_ownership_score(token)
            lp_security = token.lp_burnt_percentage / 100
            
            # Skaiƒçiuojame bendrƒÖ security risk
            security_risk = 1.0 - ((contract_security + ownership_security + lp_security) / 3)
            
            # Pridedame papildomƒÖ rizikƒÖ jei yra mint arba freeze
            if token.mint_enabled:
                security_risk += 0.3
            if token.freeze_enabled:
                security_risk += 0.2
                
            return min(security_risk, 1.0)  # Normalizuojame iki 1.0
        except:
            return 1.0

    def _assess_holder_risk(self, token: TokenMetrics) -> float:
        """Vertina holder'i≈≥ rizikƒÖ"""
        try:
            # Vertiname pagal kelis faktorius
            concentration_risk = token.top_holder_percentage / 100
            holder_count_factor = 1.0 - min(token.holders_count / 1000, 1.0)  # Normalizuojame iki 1000 holders
            sniper_risk = token.sniper_percentage / 100
            
            # Skaiƒçiuojame svertinƒØ vidurkƒØ
            weights = {'concentration': 0.4, 'count': 0.3, 'sniper': 0.3}
            
            risk_score = (
                concentration_risk * weights['concentration'] +
                holder_count_factor * weights['count'] +
                sniper_risk * weights['sniper']
            )
            
            return min(risk_score, 1.0)
        except:
            return 1.0
                    

class GemFinder:
    def __init__(self):
        self.logger = logger
        self.telegram = None
        self.scanner_client = None
        self.db_manager = None
        self.token_analyzer = None
        self.ml_analyzer = None
        self.token_handler = None
        self.processed_messages = set()

    async def initialize(self):
        """Inicializuoja visus komponentus"""
        try:
            # Sukuriame telegram klientus ir i≈° karto juos startuojame
            logger.info(f"[2025-02-03 17:21:25] Initializing Telegram clients...")
            
            self.telegram = TelegramClient('gem_finder_session', 
                                         Config.TELEGRAM_API_ID, 
                                         Config.TELEGRAM_API_HASH)
            
            self.scanner_client = TelegramClient('scanner_session',
                                               Config.TELEGRAM_API_ID,
                                               Config.TELEGRAM_API_HASH)
            
            # Sukuriame alert klientƒÖ i≈° karto
            self.alert_client = TelegramClient('gem_alert_session',
                                             Config.TELEGRAM_API_ID,
                                             Config.TELEGRAM_API_HASH)
            
            # Startuojame visus klientus
            await self.telegram.start()
            await self.scanner_client.start()
            await self.alert_client.start()
            
            logger.info(f"[2025-02-03 17:21:25] All Telegram clients initialized")

            # Pirma inicializuojame DB
            self.db_manager = DatabaseManager()
            await self.db_manager.setup_database()
            logger.info(f"[2025-02-03 17:21:25] Database initialized")
            
            # Tada kuriame kitus komponentus
            self.token_analyzer = TokenAnalyzer(self.db_manager, None)
            self.ml_analyzer = MLAnalyzer(self.db_manager, self.token_analyzer)
            self.token_analyzer.ml = self.ml_analyzer
            self.token_handler = TokenHandler(self.db_manager, self.ml_analyzer)
            
            # Perduodame jau inicializuotƒÖ alert klientƒÖ
            self.token_handler.telegram_client = self.alert_client
            
            logger.info(f"[2025-02-03 17:21:25] GemFinder initialized")

        except Exception as e:
            logger.error(f"[2025-02-03 17:21:25] Error in initialization: {e}")
            raise

    async def start(self):
        """Paleid≈æia GemFinder"""
        try:
            # Pirma inicializuojame visus komponentus
            await self.initialize()
            
            logger.info(f"[2025-02-03 17:21:25] GemFinder started")
            
            # Pradedame periodinƒØ neaktyvi≈≥ token≈≥ tikrinimƒÖ
            asyncio.create_task(self._run_periodic_checks())
            
            # Patikriname duomen≈≥ bazƒós b≈´senƒÖ
            await self.db_manager.check_database()
            
            
            
            # Registruojame message handler'ƒØ
            @self.telegram.on(events.NewMessage(chats=Config.TELEGRAM_SOURCE_CHATS))
            async def message_handler(event):
                await self._handle_message(event)
                
            # Laukiame prane≈°im≈≥
            await self.telegram.run_until_disconnected()
            
        except Exception as e:
            logger.error(f"[2025-02-03 17:21:25] Error starting GemFinder: {e}")
            raise

    async def _run_periodic_checks(self):
        """Atnaujina DB ir ML modelƒØ periodi≈°kai"""
        while True:
            try:
                logger.info(f"[2025-02-03 18:07:45] Starting database and ML update...")
                
                # 1. Patikriname ir pa≈æymime neaktyvius tokenus
                await self.token_handler.check_inactive_tokens()
                
                # 2. Gauname visus tokenus i≈° DB
                active_tokens = await self.db_manager.get_active_tokens()
                successful_tokens = await self.db_manager.get_all_gems()
                failed_tokens = await self.db_manager.get_failed_tokens()
                
                # 3. Patikriname ar aktyv≈´s tokenai tapo gems
                for token in active_tokens:
                    try:
                        if not await self.db_manager.is_gem(token['address']):
                            # Gauname tik paskutinƒØ atnaujinimƒÖ i≈° DB
                            latest_update = await self.db_manager.get_latest_token_update(token['address'])
                            initial_data = await self.db_manager.get_initial_state(token['address'])
                            
                            if latest_update and initial_data:
                                multiplier = latest_update['market_cap'] / initial_data.market_cap
                                if multiplier >= 10:
                                    await self.db_manager.mark_as_gem(token['address'])
                                    logger.info(f"[2025-02-03 18:07:45] New gem found: {token['address']} ({multiplier:.2f}X)")
                    except Exception as e:
                        logger.error(f"[2025-02-03 18:07:45] Error checking token status: {e}")
                
                # 4. Atnaujiname ML modelƒØ su duomenimis i≈° DB
                
                await self.ml_analyzer.train_model()
                
                # 5. Atnaujiname duomen≈≥ bazƒós b≈´senƒÖ
                await self.db_manager.check_database()
                
                # Naudojame RETRAIN_INTERVAL i≈° Config (valandos ƒØ sekundes)
                await asyncio.sleep(Config.RETRAIN_INTERVAL * 3600)
                
            except Exception as e:
                logger.error(f"[2025-02-03 18:07:45] Error in periodic checks: {e}")
                await asyncio.sleep(60)

    async def stop(self):
        """Sustabdo GemFinder"""
        await self.telegram.disconnect()
        await self.scanner_client.disconnect()
        logger.info(f"[2025-01-31 13:08:54] GemFinder stopped")
        
    async def _handle_message(self, event: events.NewMessage.Event):
        try:
            message = event.message.text
            message_id = event.message.id
            
            if message_id in self.processed_messages:
                return
                
            logger.info(f"[2025-01-31 13:08:54] Processing message ID: {message_id}")
            
            # Patikriname ar tai naujas token'as
            is_new_token = bool("üî•" in message and "New Trending" in message)
            
            token_addresses = self._extract_token_addresses(message)
            if not token_addresses:
                return
                
            token_address = token_addresses[0]

            # Pridedame cooldown patikrinimƒÖ
            if hasattr(self, '_recently_processed_tokens'):
                if token_address in self._recently_processed_tokens:
                    if time.time() - self._recently_processed_tokens[token_address] < 10:  # 10 sekund≈æi≈≥ cooldown
                        logger.info(f"Skipping recently processed token: {token_address}")
                        return
            else:
                self._recently_processed_tokens = {}
                
            current_data = await self._collect_token_data(token_address)
            if not current_data:
                return
            
            # Surenkame Syrax duomenis tik i≈° Syrax Scanner atsakymo
            if current_data and current_data.same_telegram_count > 0 and "from" not in message:  # Pridƒótas "from" patikrinimas
                syrax_data = {
                    'same_name_count': current_data.same_name_count,
                    'same_website_count': current_data.same_website_count,
                    'same_telegram_count': current_data.same_telegram_count,
                    'same_twitter_count': current_data.same_twitter_count
                }
                # Patikriname similarity metrikas per TokenHandler
                await self.token_handler.check_syrax_metrics(token_address, syrax_data)
                logger.info(f"Checked Syrax metrics for {token_address}")
                
            if "New" in message or is_new_token:
                await self.token_handler.handle_new_token(current_data)
            else:
                await self.token_handler.handle_token_update(token_address, current_data, is_new_token=is_new_token)
            
            # Atnaujiname paskutinio apdorojimo laikƒÖ
            self._recently_processed_tokens[token_address] = time.time()
            self.processed_messages.add(message_id)
            
        except Exception as e:
            logger.error(f"[2025-01-31 13:08:54] Error handling message: {e}")
            
    async def _handle_new_token(self, message: str):
        """Apdoroja naujƒÖ token'ƒÖ"""
        try:
            # I≈°traukiame token adresƒÖ
            token_addresses = self._extract_token_addresses(message)
            if not token_addresses:
                return

            token_address = token_addresses[0]
            logger.info(f"[2025-02-03 14:25:39] Processing new token: {token_address}")

            # Renkame token info
            token_data = await self._collect_token_data(token_address)
            if not token_data:
                return

            
            # Apdorojame naujƒÖ tokenƒÖ per TokenHandler
            initial_prediction = await self.token_handler.handle_new_token(token_data)
            
            
            
            logger.info(f"[2025-02-03 14:25:39] Token {token_address} processed with initial prediction: {initial_prediction:.2f}")

        except Exception as e:
            logger.error(f"[2025-02-03 14:25:39] Error handling new token: {e}")

    async def _handle_token_update(self, message: str):
        try:
            token_addresses = self._extract_token_addresses(message)
            if not token_addresses:
                return

            token_address = token_addresses[0]
            current_data = await self._collect_token_data(token_address)
            if not current_data:
                return

            # Perduodame update'ƒÖ TokenHandler'iui
            await self.token_handler.handle_token_update(token_address, current_data)
            logger.info(f"[{datetime.now(timezone.utc)}] Token update handled through TokenHandler: {token_address}")
                
        except Exception as e:
            logger.error(f"[{datetime.now(timezone.utc)}] Error handling token update: {e}")

    def _extract_token_addresses(self, message: str) -> List[str]:
        """I≈°traukia token adresus i≈° ≈æinutƒós"""
        matches = []
        
        try:
            # Ie≈°kome token adreso URL'uose
            if "from" in message:  # Update ≈æinutƒó
                # Ie≈°kome soul_scanner_bot URL
                scanner_matches = re.findall(r'soul_scanner_bot/chart\?startapp=([A-Za-z0-9]{32,44})', message)
                if scanner_matches:
                    matches.extend(scanner_matches)
                    
            elif "New" in message:  # Nauja ≈æinutƒó
                # Ie≈°kome soul_sniper_bot ir soul_scanner_bot URL
                patterns = [
                    r'soul_sniper_bot\?start=\d+_([A-Za-z0-9]{32,44})',
                    r'soul_scanner_bot/chart\?startapp=([A-Za-z0-9]{32,44})'
                ]
                
                for pattern in patterns:
                    url_matches = re.findall(pattern, message)
                    if url_matches:
                        matches.extend(url_matches)
            
            # Pa≈°aliname dublikatus ir filtruojame
            unique_matches = list(set(matches))
            valid_matches = [addr for addr in unique_matches if len(addr) >= 32 and len(addr) <= 44]
            
            if valid_matches:
                logger.info(f"[2025-01-31 13:14:41] Found token address: {valid_matches[0]}")
            
            return valid_matches
            
        except Exception as e:
            logger.error(f"[2025-01-31 13:14:41] Error extracting token address: {e}")
            return []

    async def _collect_token_data(self, token_address: str) -> Optional[TokenMetrics]:
        """Renka informacijƒÖ apie token'ƒÖ i≈° vis≈≥ ≈°altini≈≥"""
        
        logger.info(f"[2025-02-08 22:44:16] Collecting data for {token_address}")
        
        try:
            # Siunƒçiame token'ƒÖ ƒØ scanner grupƒô
            original_message = await self.scanner_client.send_message(
                Config.SCANNER_GROUP,
                token_address
            )
            logger.info(f"[2025-02-08 22:44:16] Sent token to scanner group: {token_address}")

            # Laukiame atsakym≈≥ i≈° bot≈≥
            timeout = 30
            start_time = time.time()
            last_check_time = 0
            soul_data = None
            syrax_data = None
            soul_responses = []  # Saugosime Soul Scanner atsakymus
            syrax_responses = []  # Saugosime Syrax Scanner atsakymus
            
            while time.time() - start_time < timeout:
                if time.time() - last_check_time >= 2:
                    last_check_time = time.time()
                    
                    async for message in self.scanner_client.iter_messages(
                        Config.SCANNER_GROUP,
                        limit=20,
                        min_id=original_message.id
                    ):
                        # Soul Scanner logika
                        if message.sender_id == Config.SOUL_SCANNER_BOT:
                            message_text = message.text
                            
                            # Patikrinimas ar ≈æinutƒóje yra m≈´s≈≥ ie≈°komas adresas
                            if token_address not in message_text:
                                continue
                            
                            # Pridedame ƒØ Soul atsakym≈≥ sƒÖra≈°ƒÖ
                            soul_responses.append(message_text)
                            
                        # Syrax Scanner logika
                        elif message.sender_id == Config.SYRAX_SCANNER_BOT:
                            message_text = message.text
                            
                            # Patikrinimas ar ≈æinutƒóje yra m≈´s≈≥ ie≈°komas adresas
                            if token_address not in message_text:
                                continue
                            
                            # Pridedame ƒØ Syrax atsakym≈≥ sƒÖra≈°ƒÖ
                            syrax_responses.append(message_text)
                    
                    # Apdorojame Soul Scanner atsakymus
                    if soul_responses and not soul_data:  # Pridƒótas not soul_data patikrinimas
                        latest_soul_message = soul_responses[-1]
                        logger.info(f"[2025-02-08 22:44:16] Soul Scanner response received (Response {len(soul_responses)} of {len(soul_responses)})")
                        soul_data = self.parse_soul_scanner_response(latest_soul_message)
                        
                        if soul_data:
                            logger.info(f"[2025-02-08 22:44:16] Soul Scanner data parsed successfully")
                            for key, value in sorted(soul_data.items()):
                                logger.info(f"[2025-02-08 22:44:16] {key}: {value}")

                    # Apdorojame Syrax Scanner atsakymus
                    if syrax_responses and not syrax_data:  # Pridƒótas not syrax_data patikrinimas
                        latest_syrax_message = syrax_responses[-1]
                        logger.info(f"[2025-02-08 22:44:16] Syrax Scanner response received (Response {len(syrax_responses)} of {len(syrax_responses)})")
                        syrax_data = self.parse_syrax_scanner_response(latest_syrax_message)
                        
                        if syrax_data:
                            logger.info(f"[2025-02-08 22:44:16] Syrax Scanner data parsed successfully")
                            for key, value in sorted(syrax_data.items()):
                                logger.info(f"[2025-02-08 22:44:16] {key}: {value}")
                            
                            # Konstruojame TokenMetrics objektƒÖ
                            token_data = TokenMetrics(
                                address=token_address,
                                name=soul_data.get('name', 'Unknown').replace('**', '').replace('\u200e', ''),
                                symbol=soul_data.get('symbol', 'Unknown'),
                                age=soul_data.get('age', '0d'),
                                total_scans=soul_data.get('total_scans', 0),
                                market_cap=soul_data.get('market_cap', 0.0),
                                liquidity=soul_data.get('liquidity', {}).get('usd', 0.0),
                                volume_1h=soul_data.get('volume', {}).get('1h', 0.0),
                                volume_24h=soul_data.get('volume', {}).get('24h', 0.0),
                                price_change_1h=soul_data.get('price_change', {}).get('1h', 0.0),
                                price_change_24h=soul_data.get('price_change', {}).get('24h', 0.0),
                                mint_enabled=1 if soul_data.get('mint_status', False) else 0,     # Pakeista ƒØ 1/0
                                freeze_enabled=1 if soul_data.get('freeze_status', False) else 0,  # 
                                lp_burnt_percentage=100 if soul_data.get('lp_status', False) else 0,
                                holders_count=soul_data.get('holders', {}).get('count', 0),
                                top_holder_percentage=soul_data.get('holders', {}).get('top_percentage', 0.0),
                                sniper_wallets=soul_data.get('sniper_wallets', []),
                                sniper_count=soul_data.get('snipers', {}).get('count', 0),
                                sniper_percentage=soul_data.get('snipers', {}).get('percentage', 0.0),
                                first_20_fresh=soul_data.get('first_20', 0),
                                
                                ath_market_cap=soul_data.get('ath_market_cap', 0.0),
                                ath_multiplier=1.0,
                                owner_renounced=1 if soul_data.get('dev', {}).get('token_percentage', 0.0) == 0 else 0,
                                telegram_url=soul_data.get('social_links', {}).get('TG'),
                                twitter_url=soul_data.get('social_links', {}).get('X'),
                                website_url=soul_data.get('social_links', {}).get('WEB'),
                                
                                dev_sol_balance=soul_data.get('dev', {}).get('sol_balance', 0.0),
                                dev_token_percentage=soul_data.get('dev', {}).get('token_percentage', 0.0),

                                # Syrax Scanner papildomi duomenys
                                dev_bought_tokens=syrax_data.get('dev_bought', {}).get('tokens', 0.0),
                                dev_bought_sol=syrax_data.get('dev_bought', {}).get('sol', 0.0),
                                dev_created_tokens=syrax_data.get('dev_created_tokens', 0),
                                same_name_count=syrax_data.get('same_name_count', 0),
                                same_website_count=syrax_data.get('same_website_count', 0),
                                same_telegram_count=syrax_data.get('same_telegram_count', 0),
                                same_twitter_count=syrax_data.get('same_twitter_count', 0),
                                bundle_count=syrax_data.get('bundle', {}).get('count', 0),
                                bundle_supply_percentage=syrax_data.get('bundle', {}).get('supply_percentage', 0.0),
                                bundle_curve_percentage=syrax_data.get('bundle', {}).get('curve_percentage', 0.0),
                                bundle_sol=syrax_data.get('bundle', {}).get('sol', 0.0)
                            )
                            
                            if token_data.market_cap > 0 and token_data.ath_market_cap > 0:
                                token_data.ath_multiplier = token_data.ath_market_cap / token_data.market_cap
                            
                            logger.info(f"[2025-01-31 12:39:29] Successfully created TokenMetrics object")
                            
                            return token_data
                    
                    await asyncio.sleep(2)
            
            logger.error(f"[2025-01-31 12:39:29] Failed to get all responses")
            return None
            
        except Exception as e:
            logger.error(f"[2025-01-31 12:39:29] Error collecting token data: {e}")
            logger.error(f"Exception traceback: {e.__traceback__.tb_lineno}")
            return None

    def clean_line(self, text: str) -> str:
        """
        I≈°valo tekstƒÖ nuo nereikaling≈≥ simboli≈≥, bet palieka svarbius emoji
        """
        
        
        important_emoji = ['üí†', 'ü§ç', '‚úÖ', '‚ùå', 'üîª', 'üêü', 'üç§', 'üê≥', 'üå±', 'üïí', 'üìà', '‚ö°Ô∏è', 'üë•', 'üîó', 'ü¶Ö', 'üî´', '‚ö†Ô∏è', 'üõ†', 'üîù', 'üî•', 'üíß', 'üò≥', 'ü§î', 'üö©', 'üì¶', 'üéØ',
            'üëç', 'üí∞', 'üíº']
        
        # 1. Pa≈°alinam Markdown ≈æymƒójimƒÖ
        cleaned = re.sub(r'\*\*', '', text)
        
        # 2. Pa≈°alinam URL formatu [text](url)
        cleaned = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', cleaned)
        
        # 3. Pa≈°alinam likusius URL skliaustus (...)
        cleaned = re.sub(r'\((?:https?:)?//[^)]+\)', '', cleaned)
    
        # Pa≈°alinam visus specialius simbolius, i≈°skyrus svarbius emoji
        result = ''
        i = 0
        while i < len(cleaned):
            if any(cleaned.startswith(emoji, i) for emoji in important_emoji):
                # Jei randame svarb≈≥ emoji, jƒØ paliekame
                emoji_found = next(emoji for emoji in important_emoji if cleaned.startswith(emoji, i))
                result += emoji_found
                i += len(emoji_found)
            else:
                # Kitaip tikriname ar tai normalus simbolis
                if cleaned[i].isalnum() or cleaned[i] in ' .:$%|-()':
                    result += cleaned[i]
                i += 1
        
        return result.strip()

    def parse_soul_scanner_response(self, text: str) -> Dict:
        """Parse Soul Scanner message"""
        try:
            data = {}
            lines = text.split('\n')

            
            
            for line in lines:
                try:
                    if not line.strip():
                        continue
                        
                    clean_line = self.clean_line(line)
                    
                    # Basic info 
                    if 'üí†' in line or 'üî•' in line:
                        parts = line.split('$')
                        data['name'] = parts[0].replace('üí†', '').replace('üî•', '').replace('‚Ä¢', '').replace('**', '').strip()
                        data['symbol'] = parts[1].replace('**', '').strip()
                            
                    # Contract Address
                    elif len(line.strip()) > 30 and not any(x in line for x in ['https://', 'üåä', 'üî´', 'üìà', 'üîó', '‚Ä¢', '‚îó', '‚î£']):
                        data['contract_address'] = line.strip().replace('`', '')
                    
                    
                    elif 'Age:' in line:
                        try:
                            number = ''.join(c for c in line if c.isdigit())
                            if number:
                                if 'm' in line:  # Jei yra "m"
                                    data['age'] = f"{number}m"  # Pridedame "m"
                                elif 'd' in line:  # Jei "d"
                                    data['age'] = f"{number}d"  # Pridedame "d"
                                elif 'h' in line:  # Jei "h"
                                    data['age'] = f"{number}h"  # Pridedame "h"
                        except Exception as e:
                            print(f"AGE ERROR: {str(e)}")
                        
                    # Market Cap and ATH
                    elif 'MC:' in line:
                        # Market Cap gali b≈´ti K arba M
                        mc_k = re.search(r'\$(\d+\.?\d*)K', clean_line)  # Ie≈°kome K
                        mc_m = re.search(r'\$(\d+\.?\d*)M', clean_line)  # Ie≈°kome M
                        
                        if mc_m:  # Jei M (milijonai)
                            data['market_cap'] = float(mc_m.group(1)) * 1000000
                        elif mc_k:  # Jei K (t≈´kstanƒçiai)
                            data['market_cap'] = float(mc_k.group(1)) * 1000
                                
                        # ATH ie≈°kojimas (po üîù)
                        ath_m = re.search(r'üîù \$(\d+\.?\d*)M', clean_line)  # Pirma tikrinam M
                        ath_k = re.search(r'üîù \$(\d+\.?\d*)K', clean_line)  # Tada K
                        
                        if ath_m:  # Jei M (milijonai)
                            data['ath_market_cap'] = float(ath_m.group(1)) * 1000000
                        elif ath_k:  # Jei K (t≈´kstanƒçiai)
                            data['ath_market_cap'] = float(ath_k.group(1)) * 1000
                    
                    # Liquidity
                    elif 'Liq:' in line:
                        liq = re.search(r'\$(\d+\.?\d*)K\s*\((\d+)\s*SOL\)', clean_line)
                        if liq:
                            data['liquidity'] = {
                                'usd': float(liq.group(1)) * 1000,
                                'sol': float(liq.group(2))
                            }
                    
                    # Volume
                    elif 'Vol:' in line:
                        
                        clean_line = self.clean_line(line)
                        
                        try:
                            data['volume'] = {'1h': 0.0, '24h': 0.0}
                            
                            parts = clean_line.split('|')
                            
                            # 1h volume
                            vol_1h = re.search(r'\$(\d+\.?\d*)(K|M)?', parts[0])
                            if vol_1h:
                                value = float(vol_1h.group(1))
                                if vol_1h.group(2) == 'K':
                                    value *= 1000
                                elif vol_1h.group(2) == 'M':
                                    value *= 1000000
                                data['volume']['1h'] = value
                                
                            
                            # 24h volume
                            if len(parts) > 1:
                                vol_24h = re.search(r'\$(\d+\.?\d*)(K|M)?', parts[1])
                                if vol_24h:
                                    value = float(vol_24h.group(1))
                                    if vol_24h.group(2) == 'K':
                                        value *= 1000
                                    elif vol_24h.group(2) == 'M':
                                        value *= 1000000
                                    data['volume']['24h'] = value
                                    
                            
                            
                                
                        except Exception as e:
                            print(f"Volume error: {str(e)}")

                    # Price Changes
                    elif 'Price:' in line:
                        
                        clean_line = self.clean_line(line)
                        
                        try:
                            parts = line.split('|')
                            data['price_change'] = {'1h': 0.0, '24h': 0.0}
                            
                            # 1h price
                            if 'üîª' in parts[0]:
                                multiplier = -1
                            else:
                                multiplier = 1
                                
                            price_1h = re.search(r'[+]?([\d.]+)(K)?%', parts[0])
                            if price_1h:
                                value = float(price_1h.group(1))
                                if price_1h.group(2) == 'K':
                                    value *= 1000
                                data['price_change']['1h'] = value * multiplier
                                
                                
                            # 24h price
                            if len(parts) > 1:
                                if 'üîª' in parts[1]:
                                    multiplier = -1
                                else:
                                    multiplier = 1
                                    
                                price_24h = re.search(r'[+]?([\d.]+)(K)?%', parts[1])
                                if price_24h:
                                    value = float(price_24h.group(1))
                                    if price_24h.group(2) == 'K':
                                        value *= 1000
                                    data['price_change']['24h'] = value * multiplier
                                    
                                    
                            
                            
                        except Exception as e:
                            print(f"Price error: {str(e)}")
                    
                    # Tikriname visƒÖ eilutƒô su Mint ir Freeze
                    elif '‚ûï Mint' in line and 'üßä Freeze' in line:
                        mint_part = line.split('|')[0]
                        freeze_part = line.split('|')[1]
                        data['mint_status'] = False if 'ü§ç' in mint_part else True
                        data['freeze_status'] = False if 'ü§ç' in freeze_part else True

                    # LP statusas - GRƒÆ≈ΩTAM PRIE TO KAS VEIKƒñ
                    elif 'LP' in line and not 'First' in line:
                        data['lp_status'] = True if 'ü§ç' in line else False

                        
                    # DEX Status
                    elif 'Dex' in line:
                        data['dex_status'] = {
                            'paid': '‚úÖ' in line,
                            'ads': not '‚ùå' in line
                        }
                    
                    # Scans
                    elif any(emoji in line for emoji in ['‚ö°', '‚ö°Ô∏è']) and 'Scans:' in line:  # Patikriname abu variantus
                        
                        try:
                            # Pa≈°alinam Markdown formatavimƒÖ ir ie≈°kome skaiƒçiaus
                            clean_line = re.sub(r'\*\*', '', line)
                            scans_match = re.search(r'Scans:\s*(\d+)', clean_line)
                            if scans_match:
                                scan_count = int(scans_match.group(1))
                                data['total_scans'] = scan_count
                                
                                
                            # Social links jau veikia teisingai, paliekame kaip yra
                            social_links = {}
                            if 'X' in line:
                                x_match = re.search(r'X\]\((https://[^)]+)\)', line)
                                if x_match:
                                    social_links['X'] = x_match.group(1)

                            if 'TG' in line:
                                tg_match = re.search(r'TG\]\((https://[^)]+)\)', line)
                                if tg_match:
                                    social_links['TG'] = tg_match.group(1)
                            
                            if 'WEB' in line:
                                web_match = re.search(r'WEB\]\((https://[^)]+)\)', line)
                                if web_match:
                                    social_links['WEB'] = web_match.group(1)
                            
                            if social_links:
                                data['social_links'] = social_links
                                
                                
                        except Exception as e:
                            print(f"Scans error: {str(e)}")  # Debug printinam klaidas
                    
                    # Holders
                    elif 'Hodls' in line:
                        
                        try:
                            # Ie≈°kome skaiƒçi≈≥ su kableliais ir procent≈≥
                            holders = re.search(r':\s*([0-9,]+)\s*‚Ä¢\s*Top:\s*([\d.]+)%', line)
                            if holders:
                                # Pa≈°alinam kablelƒØ i≈° holder count
                                holder_count = int(holders.group(1).replace(',', ''))
                                top_percent = float(holders.group(2))
                                data['holders'] = {
                                    'count': holder_count,
                                    'top_percentage': top_percent
                                }
                                
                        except Exception as e:
                            print(f"Holders error: {str(e)}")
        
                    # Snipers
                    elif 'üî´' in line and 'Snipers:' in line:
                        
                        clean_line = re.sub(r'\*\*', '', line)  # Pa≈°alinam Markdown
                        try:
                            # Ie≈°kome tik skaiƒçi≈≥ ir procento, ignoruojam ‚ö†Ô∏è
                            snipers_match = re.search(r'Snipers:\s*(\d+)\s*‚Ä¢\s*([\d.]+)%', clean_line)
                            if snipers_match:
                                data['snipers'] = {
                                    'count': int(snipers_match.group(1)),
                                    'percentage': float(snipers_match.group(2))
                                }
                                
                        except Exception as e:
                            print(f"Snipers error: {str(e)}")
                    
                    # First 20
                    elif 'First 20' in line:
                        fresh = re.search(r':\s*(\d+)\s*Fresh', clean_line)
                        if fresh:
                            data['first_20'] = int(fresh.group(1))
                    
                    # Sniper Wallets
                    elif any(emoji in line for emoji in ['üêü', 'üç§', 'üê≥', 'üå±']):
                        if 'sniper_wallets' not in data:
                            data['sniper_wallets'] = []
                        
                        matches = re.finditer(r'(üêü|üç§|üê≥|üå±).*?solscan\.io/account/([A-Za-z0-9]+)', line)
                        for match in matches:
                            data['sniper_wallets'].append({
                                'type': match.group(1),
                                'address': match.group(2)
                            })
                    
                    # Dev Info
                    elif 'üõ†Ô∏è Dev' in line:
                        
                        try:
                            if 'dev' not in data:
                                data['dev'] = {
                                    'sol_balance': 0.0,
                                    'token_percentage': 0.0,
                                    'token_symbol': '',
                                    'sniped_percentage': 0.0,
                                    'sold_percentage': 0.0,
                                    'airdrop_percentage': 0.0,
                                    'tokens_made': 0,
                                    'bonds': 0,
                                    'best_mc': 0.0
                                }
                            
                            # Pa≈°alinam Markdown formatavimƒÖ ir URL
                            clean_line = re.sub(r'\*\*|\[|\]|\(https?://[^)]+\)', '', line)
                            
                            
                            # Ie≈°kome SOL ir token info
                            dev_match = re.search(r'Dev:?\s*(\d+)\s*SOL\s*\|\s*(\d+)%\s*\$(\w+)', clean_line)
                            if dev_match:
                                data['dev'].update({
                                    'sol_balance': float(dev_match.group(1)),
                                    'token_percentage': float(dev_match.group(2)),
                                    'token_symbol': dev_match.group(3)
                                })
                                
                                
                        except Exception as e:
                            print(f"Dev main info error: {str(e)}")

                    # Sniped ir Sold info
                    elif 'Sniped:' in line:
                        
                        try:
                            if 'dev' not in data:
                                data['dev'] = {}
                            
                            sniped = re.search(r'Sniped:\s*([\d.]+)%', clean_line)
                            if sniped:
                                data['dev']['sniped_percentage'] = float(sniped.group(1))
                                
                            
                            sold = re.search(r'Sold:\s*([\d.]+)%', clean_line)
                            if sold:
                                data['dev']['sold_percentage'] = float(sold.group(1))
                                
                                
                        except Exception as e:
                            print(f"Sniped/Sold info error: {str(e)}")

                    # Airdrop info
                    elif 'Airdrop:' in line:
                        
                        try:
                            if 'dev' not in data:
                                data['dev'] = {}
                            
                            airdrop = re.search(r'Airdrop:\s*(\d+)%', clean_line)
                            if airdrop:
                                data['dev']['airdrop_percentage'] = float(airdrop.group(1))
                                
                                
                        except Exception as e:
                            print(f"Airdrop info error: {str(e)}")

                    # Made, Bond, Best info
                    elif 'Made:' in line:
                        
                        try:
                            if 'dev' not in data:
                                data['dev'] = {}
                                
                            made = re.search(r'Made:\s*(\d+)', line)
                            bond = re.search(r'Bond:\s*(\d+)', line)
                            best = re.search(r'Best:\s*\$(\d+\.?\d*)M', line)
                            
                            if made and bond and best:
                                data['dev'].update({
                                    'tokens_made': int(made.group(1)),
                                    'bonds': int(bond.group(1)),
                                    'best_mc': float(best.group(1)) * 1000000
                                })
                                
                                
                        except Exception as e:
                            print(f"Made/Bond/Best info error: {str(e)}")
        
                except Exception as e:
                    self.logger.warning(f"Error parsing line: {str(e)}")
                    continue
            
            return data

        except Exception as e:
            self.logger.error(f"Error parsing message: {str(e)}")
            return {}

    def parse_syrax_scanner_response(self, text: str) -> Dict:
        """Parse Syrax Scanner message"""
        try:
            data = {
                'dev_bought': {'tokens': 0.0, 'sol': 0.0, 'percentage': 0.0, 'curve_percentage': 0.0},
                'dev_created_tokens': 0,
                'same_name_count': 0,
                'same_website_count': 0,
                'same_telegram_count': 0,
                'same_twitter_count': 0,
                'bundle': {'count': 0, 'supply_percentage': 0.0, 'curve_percentage': 0.0, 'sol': 0.0},
                'notable_bundle': {'count': 0, 'supply_percentage': 0.0, 'curve_percentage': 0.0, 'sol': 0.0},  # Naujas
                'sniper_activity': {'tokens': 0.0, 'percentage': 0.0, 'sol': 0.0}
            }

            if not text:
                logger.warning(f"[2025-02-09 10:37:11] Empty text received")
                return data
                
            lines = text.split('\n')
            
            for line in lines:
                try:
                    clean_line = self.clean_line(line)
                    
                    # Dev bought info
                    if 'Dev bought' in clean_line:
                        tokens_match = re.search(r'(\d+\.?\d*)([KMB]) tokens', clean_line)
                        sol_match = re.search(r'(\d+\.?\d*) SOL', clean_line)
                        percentage_match = re.search(r'(\d+\.?\d*)%', clean_line)
                        curve_match = re.search(r'(\d+\.?\d*)% of curve', clean_line)
                        
                        if tokens_match:
                            value = float(tokens_match.group(1))
                            multiplier = {'K': 1000, 'M': 1000000, 'B': 1000000000}[tokens_match.group(2)]
                            data['dev_bought']['tokens'] = value * multiplier
                        if sol_match:
                            data['dev_bought']['sol'] = float(sol_match.group(1))
                        if percentage_match:
                            data['dev_bought']['percentage'] = float(percentage_match.group(1))
                        if curve_match:
                            data['dev_bought']['curve_percentage'] = float(curve_match.group(1))
                    
                    # Bundle info (üö© Bundled!)
                    if 'üö©' in clean_line and 'Bundled' in clean_line:
                        count_match = re.search(r'(\d+) trades', clean_line)
                        supply_match = re.search(r'(\d+\.?\d*)%', clean_line)
                        curve_match = re.search(r'\((\d+\.?\d*)% of curve\)', clean_line)
                        sol_match = re.search(r'(\d+\.?\d*) SOL', clean_line)
                        
                        if count_match:
                            data['bundle']['count'] = int(count_match.group(1))
                        if supply_match:
                            data['bundle']['supply_percentage'] = float(supply_match.group(1))
                        if curve_match:
                            data['bundle']['curve_percentage'] = float(curve_match.group(1))
                        if sol_match:
                            data['bundle']['sol'] = float(sol_match.group(1))
                    
                    # Notable bundle info (üì¶ notable bundle(s))
                    if 'üì¶' in clean_line and 'notable bundle' in clean_line:
                        # 1. PIRMA i≈°valom URL su skliaustais ir kablelius po j≈≥
                        clean_text = re.sub(r'\(http[^)]+\),', '', clean_line)
                        

                        # 2. TADA naudojam TUOS PAƒåIUS regex'us kaip bundle ir sniper
                        count_match = re.search(r'üì¶\s*(\d+)\s*notable', clean_text)
                        supply_match = re.search(r'(\d+\.?\d*)%\s*of\s*supply', clean_text)
                        curve_match = re.search(r'\((\d+\.?\d*)%\s*of\s*curve\)', clean_text)
                        sol_match = re.search(r'(\d+\.?\d*)\s*SOL', clean_text)
                      
                        if count_match:
                            data['notable_bundle']['count'] = int(count_match.group(1))
                        if supply_match:
                            data['notable_bundle']['supply_percentage'] = float(supply_match.group(1))
                        if curve_match:
                            data['notable_bundle']['curve_percentage'] = float(curve_match.group(1))
                        if sol_match:
                            data['notable_bundle']['sol'] = float(sol_match.group(1))
                            
                    # Sniper activity
                    if 'üéØ' in clean_line and 'Notable sniper activity' in clean_line:
                        tokens_match = re.search(r'(\d+\.?\d*)M', clean_line)           # tik M skaiƒçiams
                        percentage_match = re.search(r'\((\d+\.?\d*)%\)', clean_line)   # bazinis regex procentams skliausteliuose 
                        sol_match = re.search(r'(\d+\.?\d*) SOL', clean_line)          # tas pats formatas visiems
                        
                        if tokens_match:
                            data['sniper_activity']['tokens'] = float(tokens_match.group(1)) * 1000000
                        if percentage_match:
                            data['sniper_activity']['percentage'] = float(percentage_match.group(1))
                        if sol_match:
                            data['sniper_activity']['sol'] = float(sol_match.group(1))
                    
                    # Dev created tokens
                    elif 'Dev created' in clean_line:
                        match = re.search(r'Dev created (\d+)', clean_line)
                        if match:
                            data['dev_created_tokens'] = int(match.group(1))
                    
                    # Same name count
                    elif 'same as' in clean_line and 'name' in clean_line.lower():
                        match = re.search(r'same as (\d+)', clean_line)
                        if match:
                            data['same_name_count'] = int(match.group(1))
                    
                    # Same website count
                    elif 'same as' in clean_line and 'website' in clean_line.lower():
                        match = re.search(r'same as (\d+)', clean_line)
                        if match:
                            data['same_website_count'] = int(match.group(1))
                    
                    # Same telegram count
                    elif 'same as' in clean_line and 'telegram' in clean_line.lower():
                        match = re.search(r'same as (\d+)', clean_line)
                        if match:
                            data['same_telegram_count'] = int(match.group(1))
                    
                    # Same twitter count
                    elif 'same as' in clean_line and 'twitter' in clean_line.lower():
                        match = re.search(r'same as (\d+)', clean_line)
                        if match:
                            data['same_twitter_count'] = int(match.group(1))
                    
                except Exception as e:
                    logger.warning(f"[2025-02-09 10:37:11] Error parsing line '{line}': {str(e)}")
                    continue

            #logger.info(f"[2025-02-09 10:37:11] Successfully parsed Syrax data: {data}")
            return data

        except Exception as e:
            logger.error(f"[2025-02-09 10:37:11] Main parsing error: {e}")
            return data
        
if __name__ == "__main__":
    try:
        gem_finder = GemFinder()
        logger.info(f"[2025-02-01 12:27:36] Starting GemFinder...")
        
        # Pakeiƒçiame get_event_loop() ƒØ run()
        asyncio.run(gem_finder.start())
        
    except KeyboardInterrupt:
        logger.info(f"[2025-02-01 12:27:36] Shutting down GemFinder...")
        # Taip pat pakeiƒçiame ir ƒçia
        asyncio.run(gem_finder.stop())
    except Exception as e:
        logger.error(f"[2025-02-01 12:27:36] Fatal error: {str(e)}")
        raise
